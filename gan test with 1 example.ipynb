{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 20 17:17:47 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.51.05    Driver Version: 450.51.05    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:5E:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  Off  | 00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from conllu import parse_incr\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "files = {'train': ['ru_syntagrus-ud-train-a.conllu', 'ru_syntagrus-ud-train-b.conllu', 'ru_syntagrus-ud-train-c.conllu'],\n",
    "         'test':  ['ru_syntagrus-ud-test.conllu'],\n",
    "         'dev':   ['ru_syntagrus-ud-dev.conllu']}\n",
    "\n",
    "labels = []\n",
    "sentences = []\n",
    "for data_type in files:\n",
    "    for filename in files[data_type]: \n",
    "        with open(os.path.join('UD_Russian-SynTagRus', filename), encoding='utf-8') as f:\n",
    "            parsed = parse_incr(f)\n",
    "            for token_list in parsed:\n",
    "                topic_name = token_list.metadata['sent_id'].split('.')[0]\n",
    "                # уберём цифры из названий темы\n",
    "                topic_name = re.sub(r'\\d+', '', topic_name)\n",
    "                sentence = ' '.join([token['form'] for token in token_list]).lower()\n",
    "                labels.append(topic_name)\n",
    "                sentences.append(sentence)\n",
    "\n",
    "le = LabelEncoder()\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(le.fit_transform(labels).reshape(-1, 1))\n",
    "\n",
    "labels = np.array(labels)\n",
    "sentences = np.array(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# оставляем только 1 пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Anketa'], dtype='<U53'),\n",
       " array(['однако стиль работы семена еремеевича заключался в том , чтобы принимать всех желающих и лично вникать в дело .'],\n",
       "       dtype='<U1218'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = labels[3:4]\n",
    "sentences = sentences[3:4]\n",
    "labels, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = -1\n",
    "lengths = []\n",
    "for message in sentences:\n",
    "    max_len = len(message.split()) if len(message.split()) > max_len else max_len\n",
    "    lengths.append(len(message.split()))\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(lengths, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LEN = 52\n",
    "EMBEDDING_SIZE = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!ЭМБЕДИНГИ НЕ КАЧЕСТВЕННЫЕ, не сдавайся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('word2vec_100.pkl', 'rb') as rf:\n",
    "    W2V_100 = pickle.load(rf)\n",
    "with open('fasttext_100.pkl', 'rb') as rf:\n",
    "    FT_100 = pickle.load(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('накрахмаленных', 0.81927490234375),\n",
       "  (';', 0.8096592426300049),\n",
       "  ('рядовыми', 0.8064565062522888),\n",
       "  ('чеченская', 0.7928946018218994),\n",
       "  ('соцстранах', 0.7922433614730835),\n",
       "  ('дымящих', 0.7861340641975403),\n",
       "  ('одиночного', 0.7822094559669495),\n",
       "  ('наркотикам', 0.7802185416221619),\n",
       "  ('линейный', 0.7785296440124512),\n",
       "  ('качественную', 0.7770684361457825)],\n",
       " [('он', 0.9201070070266724),\n",
       "  ('уж', 0.8810179829597473),\n",
       "  ('мыл', 0.8596895933151245),\n",
       "  ('мышь', 0.8507342338562012),\n",
       "  ('онф', 0.8464065194129944),\n",
       "  ('мне', 0.8424739241600037),\n",
       "  ('даешь', 0.838414192199707),\n",
       "  ('он-лайн', 0.8318077921867371),\n",
       "  ('ешь', 0.8273506760597229),\n",
       "  ('да', 0.8267568349838257)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2V_100.wv.most_similar(positive='.'), FT_100.wv.most_similar('я')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequence, target_len: int=20, embedding_size: int=300) -> np.array:\n",
    "    sequence = np.array(sequence)\n",
    "    if sequence.size == 0:\n",
    "        # empty array\n",
    "        current_length = 0\n",
    "        return np.zeros((target_len, embedding_size))\n",
    "    elif len(sequence.shape) == 1:\n",
    "        sequence = np.array([sequence])\n",
    "        current_length = 1\n",
    "    else:\n",
    "        current_length = sequence.shape[0]\n",
    "        \n",
    "    if current_length >= target_len:\n",
    "        return sequence[-target_len:]\n",
    "    \n",
    "    # padding = np.random.uniform(size=(target_len - current_length, embedding_size))\n",
    "    padding = np.zeros((target_len - current_length, embedding_size))\n",
    "    return np.concatenate((padding, sequence), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def vectorize_sentence(sentence):\n",
    "    vec = []\n",
    "    for token in sentence.split():\n",
    "        tkn_vec = np.concatenate((W2V_100.wv[token], FT_100.wv[token]), axis=0)\n",
    "        vec.append(tkn_vec)\n",
    "    return pad_sequences(vec, target_len=TARGET_LEN, embedding_size=EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['однако стиль работы семена еремеевича заключался в том , чтобы принимать всех желающих и лично вникать в дело .',\n",
       "        'однако стиль работы семена еремеевича заключался в том , чтобы принимать всех желающих и лично вникать в дело .',\n",
       "        'однако стиль работы семена еремеевича заключался в том , чтобы принимать всех желающих и лично вникать в дело .'],\n",
       "       dtype='<U1218'),\n",
       " array(['Anketa', 'Anketa', 'Anketa'], dtype='<U53')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_DIM = 100\n",
    "\n",
    "\n",
    "class DataGenerator():\n",
    "    def __init__(self, labels, sentences, batches_per_epoch, batch_size):\n",
    "        self.train_sentences = sentences\n",
    "        self.train_labels = labels\n",
    "        self.batches_per_epoch = batches_per_epoch\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    @staticmethod\n",
    "    def rand_norm(npoints=1, ndim=RANDOM_DIM):\n",
    "        rand_vec = np.random.normal(0, 1, size=[npoints, ndim])\n",
    "        return rand_vec\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batches_per_epoch\n",
    "\n",
    "    def batch(self):\n",
    "        rand_sentences_indexes = np.random.randint(0, self.train_sentences.shape[0], size=self.batch_size)\n",
    "        sent_batch = np.array([vectorize_sentence(sent) for sent in self.train_sentences[rand_sentences_indexes]])\n",
    "        labels_batch = ohe.transform(le.transform(self.train_labels[rand_sentences_indexes]).reshape(-1, 1))\n",
    "        return sent_batch, labels_batch\n",
    "\n",
    "    def __iter__(self):\n",
    "        for b in tqdm(range(self.batches_per_epoch), leave=False):\n",
    "            yield self.batch()\n",
    "\n",
    "    def rand_batch(self):\n",
    "        rand_vec = self.rand_norm(self.batch_size)\n",
    "        # for training generator with label\n",
    "        rand_labels = np.random.randint(0, len(le.classes_), size=[self.batch_size])\n",
    "        rand_labels = ohe.transform(rand_labels.reshape(-1, 1)).todense()\n",
    "        return (rand_vec, rand_labels)\n",
    "\n",
    "    \n",
    "np.random.seed(42)\n",
    "sent_indexes = np.random.randint(0, len(sentences), size=3)\n",
    "MESSAGES_FROM_DATASET = [sentences[sent_indexes], labels[sent_indexes]]\n",
    "FIXED_NOISE = (DataGenerator.rand_norm(3), ohe.transform(le.transform(labels[sent_indexes]).reshape(-1, 1)).todense())\n",
    "MESSAGES_FROM_DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0998,  0.9477, -0.5227,  0.7144, -0.8415],\n",
       "         [ 0.0998,  0.9477,  0.5227,  0.7144,  0.8415]]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def give_pe(batch_size, seq_len, features):\n",
    "    '''positiona encoding'''\n",
    "    # create constant 'pe' matrix with values dependant on \n",
    "    # seq_length and features\n",
    "    pe = torch.linspace(-1, 1, seq_len)\n",
    "    pe = pe.repeat(batch_size, features, 1)\n",
    "    pe = torch.transpose(pe, dim0=1, dim1=2)\n",
    "    position = torch.linspace(0.1, 1, features)\n",
    "    position = position.repeat(batch_size, seq_len, 1)\n",
    "    pe = torch.mul(pe, position)\n",
    "    pe[:, :, 0::2] = torch.sin(pe[:, :, 0::2])\n",
    "    pe[:, :, 1::2] = torch.cos(pe[:, :, 1::2])\n",
    "    return pe\n",
    "\n",
    "\n",
    "give_pe(1, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainable params: 1801544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 52, 200])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, complexity=180):\n",
    "        super(Generator, self).__init__()\n",
    "        self.complexity = int(complexity)\n",
    "        # LAYER 1\n",
    "        self.fc_1 = torch.nn.Linear(\n",
    "            in_features=RANDOM_DIM + len(le.classes_),\n",
    "            out_features=TARGET_LEN//2//2*self.complexity,\n",
    "        )\n",
    "        self.init_small_std(self.fc_1)\n",
    "        self.activation_1 = torch.nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "        # LAYER 2\n",
    "        self.con1dT_2 = torch.nn.ConvTranspose1d(\n",
    "            in_channels=TARGET_LEN//2//2,\n",
    "            out_channels=TARGET_LEN//2,\n",
    "            kernel_size=3\n",
    "        )\n",
    "        self.init_small_std(self.con1dT_2)\n",
    "        self.batch_norm_2 = torch.nn.BatchNorm1d(\n",
    "            num_features=TARGET_LEN//2\n",
    "        )\n",
    "        self.activation_2 = torch.nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "        # LAYER 3\n",
    "        self.con1dT_3 = torch.nn.ConvTranspose1d(\n",
    "            in_channels=TARGET_LEN//2,\n",
    "            out_channels=TARGET_LEN,\n",
    "            kernel_size=3\n",
    "        )\n",
    "        self.init_small_std(self.con1dT_3)\n",
    "        self.batch_norm_3 = torch.nn.BatchNorm1d(\n",
    "            num_features=TARGET_LEN\n",
    "        )\n",
    "        self.activation_3 = torch.nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "        # LAYER 4\n",
    "        self.pe = give_pe(1, TARGET_LEN, self.complexity + 4)\n",
    "#         self.lstm_4 = torch.nn.LSTM(\n",
    "#             input_size=(self.complexity + 4)*2,\n",
    "#             hidden_size=self.complexity,\n",
    "#             bidirectional=True,\n",
    "#         )\n",
    "#         self.init_small_std(self.lstm_4)\n",
    "#         self.batch_norm_4 = torch.nn.BatchNorm1d(\n",
    "#             num_features=TARGET_LEN\n",
    "#         )\n",
    "#         self.activation_4 = torch.nn.Sigmoid()\n",
    "        \n",
    "        # LAYER 5\n",
    "        self.fc_5 = torch.nn.Linear(\n",
    "            #in_features=self.complexity * 2,\n",
    "            in_features=(self.complexity + 4)*2,\n",
    "            out_features=self.complexity * 2,\n",
    "        )\n",
    "        self.init_small_std(self.fc_5)\n",
    "        self.activation_5 = torch.nn.LeakyReLU()\n",
    "        \n",
    "        # LAYER 6\n",
    "        self.fc_6 = torch.nn.Linear(\n",
    "            in_features=self.complexity * 2,\n",
    "            out_features=EMBEDDING_SIZE,\n",
    "        )\n",
    "        self.init_small_std(self.fc_6)\n",
    "\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=0.01)\n",
    "        # self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        self.is_real_loss = torch.nn.BCELoss()\n",
    "        self.category_loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        print(\n",
    "            'total trainable params:',\n",
    "            sum(p.numel() for p in self.parameters() if p.requires_grad),\n",
    "        )\n",
    "\n",
    "    def init_small_std(self, nn_module):\n",
    "        for p in nn_module.parameters():\n",
    "            torch.nn.init.normal_(p, std=0.02)\n",
    "\n",
    "    def forward(self, random_vec, labels):\n",
    "        random_vec_labels = torch.Tensor(random_vec), torch.Tensor(labels)\n",
    "        inp = torch.cat(random_vec_labels, 1)\n",
    "        \n",
    "        # LAYER 1\n",
    "        # (batch_size x (RANDOM_DIM + ohe_classes))\n",
    "        inp = self.fc_1(inp)\n",
    "        # (batch_size x (complexity * TARGET_LEN/2/2))\n",
    "        inp = self.activation_1(inp)\n",
    "        # (batch_size x (complexity * TARGET_LEN/2/2))\n",
    "        inp=torch.reshape(inp, (inp.shape[0], TARGET_LEN//2//2, self.complexity))\n",
    "        \n",
    "        # LAYER 2\n",
    "        # (batch_size x 13 x complexity)\n",
    "        inp = self.con1dT_2(inp)\n",
    "        # (batch_size x 26 x 66)\n",
    "        inp = self.batch_norm_2(inp)\n",
    "        # (batch_size x 26 x 66)\n",
    "        inp = self.activation_2(inp)\n",
    "        \n",
    "        # LAYER 3\n",
    "        # (batch_size x 26 x complexity + 2)\n",
    "        inp = self.con1dT_3(inp)\n",
    "        # (batch_size x 52 x complexity + 4)\n",
    "        inp = self.batch_norm_3(inp)\n",
    "        # (batch_size x 52 x complexity + 4)\n",
    "        inp = self.activation_3(inp)\n",
    "        \n",
    "        \n",
    "        # LAYER 4\n",
    "        # (batch_size x 52 x complexity + 4)\n",
    "        inp = torch.cat((inp, self.pe.repeat((len(inp), 1, 1))), 2)\n",
    "#         inp, _ = self.lstm_4(inp)\n",
    "#         # (batch_size x 52 x 2*complexity)\n",
    "#         inp = self.batch_norm_4(inp)\n",
    "#         # (batch_size x 52 x 2*complexity)\n",
    "#         inp = self.activation_4(inp)\n",
    "        \n",
    "        # LAYER 5\n",
    "        # (batch_size x 52 x 2*complexity)\n",
    "        inp = self.fc_5(inp)\n",
    "        inp = self.activation_5(inp)\n",
    "        \n",
    "        # LAYER 6\n",
    "        # (batch_size x 52 x 2*complexity)\n",
    "        inp = self.fc_6(inp)\n",
    "        \n",
    "        return inp\n",
    "\n",
    "generator = Generator()\n",
    "generated_from_noise = generator.forward(*FIXED_NOISE)\n",
    "generated_from_noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('перескажем перескажем выведенных добывают рыбозаградители перескажем ответчиков стелы добывают добывают', 1.1463493820673858)\n",
      "('ответчиков ответчиков ответчиков соберешь оборонку рыбозаградители возвратившихся разрешающих стелы тяпнули', 1.1664996430134544)\n",
      "('андерс рыбозаградители интервала добывают разрешающих соломка эрдельтерьеров выражениям перескажем рыбозаградители', 1.1497113397076295)\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "def message_recovery_with_metric(vector: np.ndarray):\n",
    "    assert len(vector.shape) == 2\n",
    "    tokens = []\n",
    "    metric = []\n",
    "    vector = vector.cpu().detach().numpy()\n",
    "    for token_vec in vector:\n",
    "        w2v = token_vec[:100]\n",
    "        token = W2V_100.wv.most_similar([w2v])[0][0]\n",
    "        tokens.append(token)\n",
    "        \n",
    "        metric.append(cosine(token_vec[100:], FT_100.wv[token]))\n",
    "    return ' '.join(tokens[-10:]), np.mean(metric)\n",
    "\n",
    "\n",
    "\n",
    "for example in generated_from_noise:\n",
    "    print(message_recovery_with_metric(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISCRIMINATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainable params: 2031760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.4971, 0.4952, 0.4980], grad_fn=<SigmoidBackward0>),\n",
       " tensor([[0.0016, 0.0016, 0.0016,  ..., 0.0019, 0.0017, 0.0017],\n",
       "         [0.0016, 0.0016, 0.0015,  ..., 0.0019, 0.0018, 0.0016],\n",
       "         [0.0016, 0.0016, 0.0016,  ..., 0.0018, 0.0018, 0.0016]],\n",
       "        grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, complexity=120):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.complexity = int(complexity)\n",
    "        # LAYER 1\n",
    "#         self.lstm_1 = torch.nn.LSTM(\n",
    "#             input_size=EMBEDDING_SIZE,\n",
    "#             hidden_size=self.complexity,\n",
    "#             bidirectional=True,\n",
    "#         )\n",
    "        #self.dropout_1 = torch.nn.Dropout2d(p=0.2)\n",
    "        #self.activation_1 = torch.nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "        # LAYER 2\n",
    "        self.conv1D_2 = torch.nn.ConvTranspose1d(\n",
    "            in_channels=TARGET_LEN,\n",
    "            out_channels=self.complexity,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "        )\n",
    "        self.dropout_2 = torch.nn.Dropout2d(p=0.2)\n",
    "        self.activation_2 = torch.nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "        # LAYER 3\n",
    "        self.conv1D_3 = torch.nn.ConvTranspose1d(\n",
    "            in_channels=self.complexity,\n",
    "            out_channels=self.complexity//2,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "        )\n",
    "        self.dropout_3 = torch.nn.Dropout2d(p=0.2)\n",
    "        self.activation_3 = torch.nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "        # LAYER 4\n",
    "        self.fc_4 = torch.nn.Linear(\n",
    "            in_features=803,#(self.complexity*4+1)*2+1,\n",
    "            out_features=self.complexity*2,\n",
    "        )\n",
    "        self.activation_4 = torch.nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.flatten_4 = torch.nn.Flatten(start_dim=1)\n",
    "        \n",
    "        # LAYER 5\n",
    "        self.fc_5 = torch.nn.Linear(\n",
    "            in_features=self.complexity*self.complexity,\n",
    "            out_features=self.complexity,\n",
    "        )\n",
    "        self.activation_5 = torch.nn.LeakyReLU(negative_slope=0.2)   \n",
    "        \n",
    "        \n",
    "        # LAYER 6_1\n",
    "        self.fc_6_1 = torch.nn.Linear(\n",
    "            in_features=self.complexity,\n",
    "            out_features=1,\n",
    "        )\n",
    "        self.flatten_6_1 = torch.nn.Flatten(start_dim=0)\n",
    "        self.activation_6_1 = torch.nn.Sigmoid()\n",
    "        \n",
    "        # LAYER 6_2\n",
    "        self.fc_6_2 = torch.nn.Linear(\n",
    "            in_features=self.complexity,\n",
    "            out_features=len(le.classes_),\n",
    "        )\n",
    "        self.activation_6_2 = torch.nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.is_real_loss = torch.nn.BCELoss()\n",
    "        self.category_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        \n",
    "        print(\n",
    "            'total trainable params:',\n",
    "            sum(p.numel() for p in self.parameters() if p.requires_grad),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, sentence_vec):\n",
    "        inp = sentence_vec\n",
    "        \n",
    "#         # LAYER 1\n",
    "#         # (batch_size x target_len x embedding_size)\n",
    "#         inp, _ = self.lstm_1(inp)\n",
    "#         # (batch_size x target_len x complexity*2)\n",
    "#         inp = self.dropout_1(inp)\n",
    "#         # (batch_size x target_len x complexity*2)\n",
    "#         inp = self.activation_1(inp)\n",
    "\n",
    "        # LAYER 2\n",
    "        # (batch_size x target_len x complexity*2)\n",
    "        inp = self.conv1D_2(inp)\n",
    "        # (batch_size x target_len/2 x complexity*2)\n",
    "        inp = self.dropout_2(inp)\n",
    "        # (batch_size x target_len/2 x complexity*2)\n",
    "        inp = self.activation_2(inp)\n",
    "        \n",
    "        # LAYER 3\n",
    "        # (batch_size x target_len x complexity*2)\n",
    "        inp = self.conv1D_3(inp)\n",
    "        # (batch_size x target_len/4 x complexity)\n",
    "        inp = self.dropout_3(inp)\n",
    "        # (batch_size x target_len/4 x complexity)\n",
    "        inp = self.activation_3(inp)\n",
    "        \n",
    "        \n",
    "        # LAYER 4\n",
    "        # (batch_size x target_len/4 x complexity)\n",
    "        inp = self.fc_4(inp)\n",
    "        # (batch_size x target_len/4 x complexity)\n",
    "        inp = self.activation_4(inp)\n",
    "        # (batch_size x target_len/4 x complexity)\n",
    "        inp = self.flatten_4(inp)\n",
    "        # (batch_size x target_len/4  complexity)\n",
    "        \n",
    "        # LAYER 5\n",
    "        # (batch_size x complexity * 2)\n",
    "        inp = self.fc_5(inp)\n",
    "        # (batch_size x complexity)\n",
    "        inp = self.activation_5(inp)\n",
    "        \n",
    "        # LAYER 6_1\n",
    "        # (batch_size x complexity)\n",
    "        inp_1 = self.fc_6_1(inp)\n",
    "        inp_1 = self.flatten_6_1(inp_1)\n",
    "        # (batch_size x 2)\n",
    "        inp_1 = self.activation_6_1(inp_1)\n",
    "        \n",
    "        # LAYER 6_2\n",
    "        # (batch_size x complexity)\n",
    "        inp_2 = self.fc_6_2(inp)\n",
    "        # (batch_size x 2)\n",
    "        inp_2 = self.activation_6_2(inp_2)\n",
    "        return inp_1, inp_2\n",
    "    \n",
    "    def train_on_batch(self, messages, y_real_fake, y_label):\n",
    "        messages = torch.Tensor(messages)\n",
    "        pred_real_fake, pred_label = self.forward(messages)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.is_real_loss(y_real_fake, pred_real_fake) + 0.01 * self.category_loss(y_label, pred_label)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        accuracy = torch.mean(((pred_real_fake > 0.5).int() == (y_real_fake > 0.5).int()).float())\n",
    "        return loss, accuracy\n",
    "\n",
    "\n",
    "discriminator = Discriminator()\n",
    "discriminator.forward(generated_from_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(torch.nn.Module):\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super(GAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        \n",
    "    def train_generator_on_batch(self, noise, y_real_fake, y_label):\n",
    "        sentences = self.generator.forward(*noise)\n",
    "        pred_real_fake, pred_label = self.discriminator.forward(sentences)\n",
    "        loss_1 = self.discriminator.is_real_loss(y_real_fake, pred_real_fake)\n",
    "        loss_2 = self.discriminator.category_loss(torch.Tensor(y_label), pred_label)\n",
    "        loss = loss_1 + 0.01 * loss_2\n",
    "        # self.generator.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.generator.optimizer.step()\n",
    "        accuracy = torch.mean(((pred_real_fake > 0.5).int() == (y_real_fake > 0.5).int()).float()).numpy()\n",
    "        return loss, accuracy\n",
    "\n",
    "gan = GAN(generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.11 0.53 0.47 grad_fn=<AddBackward0>) \t tensor(0.5263) \t tensor(0.4737) \t tensor(0.1180, grad_fn=<AddBackward0>) \t tensor(0.5254) \t tensor(0.4746) \t\n",
      "('рублевском рублевском рублевском рублевском рублевском рублевском рублевском рублевском рублевском рублевском', 1.1128135567280248)\n",
      "('рублевском рублевском рублевском рублевском рублевском рублевском рублевском рублевском рублевском рублевском', 1.1210071240202524)\n",
      "('рублевском рублевском рублевском рублевском рублевском рублевском рублевском рублевском рублевском рублевском', 1.1123164914942418)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.1 0.53 0.47, grad_fn=<AddBackward0>) \t tensor(0.5263) \t tensor(0.4737) \ttensor(0.5263) \t tensor(0.4737) \ttensor(0.5263) \t tensor(0.4737) \ttensor(0.1141, grad_fn=<AddBackward0>) \t tensor(0.5263) \t tensor(0.4737) \t\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6502c6fd2e406f971e15f0849e4fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.15 0.53 0.47 grad_fn=<AddBackward0>) \t tensor(0.5263) \t tensor(0.4737) \t\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-ee6a906247da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         ), 0)        \n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_real_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# print('1', loss, accuracy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdiscriminator_accuracy_cumulative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdiscriminator_accuracy_cumulative\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-b6d4091bad45>\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, messages, y_real_fake, y_label)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_real_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mpred_real_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_real_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_real_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_real_fake\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.01\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-b6d4091bad45>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence_vec)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# LAYER 5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# (batch_size x complexity * 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# (batch_size x complexity)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generator_acc_follow = []\n",
    "generator_accuracy_cumulative = 0.5\n",
    "discriminator_accuracy_cumulative = 0.5\n",
    "loss_cumulative = 1\n",
    "BATCH_SIZE = 2\n",
    "data = DataGenerator(labels, sentences, batches_per_epoch=300, batch_size=BATCH_SIZE)\n",
    "\n",
    "for epoch in range(30):\n",
    "    for sent_batch, labels_batch in data:\n",
    "        ##########################\n",
    "        # discriminator training #\n",
    "        ##########################\n",
    "        noise = data.rand_batch()\n",
    "        generated_texts = generator.forward(*noise)\n",
    "        # X\n",
    "        X = torch.cat((torch.Tensor(sent_batch), generated_texts), 0)\n",
    "        # target valuest smoothing\n",
    "        y_real = np.random.uniform(0.8, 1, size=[BATCH_SIZE])\n",
    "        y_fake = np.random.uniform(0, 0.2, size=[BATCH_SIZE])\n",
    "        # Y\n",
    "        y_real_fake = torch.Tensor(np.concatenate((y_real, y_fake)))\n",
    "        y_labels = torch.cat((\n",
    "            torch.Tensor(labels_batch.todense()),\n",
    "            torch.Tensor(np.zeros((BATCH_SIZE, len(le.classes_)))),\n",
    "        ), 0)        \n",
    "        \n",
    "        loss, accuracy = discriminator.train_on_batch(X, y_real_fake, y_labels)\n",
    "        # print('1', loss, accuracy)\n",
    "        discriminator_accuracy_cumulative = 0.9 * discriminator_accuracy_cumulative + 0.1 * accuracy\n",
    "        generator_accuracy_cumulative = 1 - discriminator_accuracy_cumulative\n",
    "        \n",
    "        ######################\n",
    "        # generator training #\n",
    "        ######################\n",
    "        while discriminator_accuracy_cumulative > 0.5:\n",
    "            noise = data.rand_batch()\n",
    "            # target values\n",
    "            y_real_fake = torch.Tensor(np.random.uniform(0.9, 1.0, size=[BATCH_SIZE]))\n",
    "\n",
    "            # train GAN and save accuracy to array\n",
    "            loss, accuracy = gan.train_generator_on_batch(noise, y_real_fake, noise[1])\n",
    "\n",
    "            generator_accuracy_cumulative = 0.9 * generator_accuracy_cumulative + 0.1 * accuracy\n",
    "            # print('2', accuracy)\n",
    "            discriminator_accuracy_cumulative = 1 - generator_accuracy_cumulative\n",
    "            print('1', loss, '\\t', generator_accuracy_cumulative, '\\t', discriminator_accuracy_cumulative, '\\t', end='\\r')\n",
    "#         for p in discriminator.parameters():\n",
    "#             print('discriminator', p[-1].shape, p[-1][0][:3])\n",
    "#             break\n",
    "                \n",
    "#             for p in generator.parameters():\n",
    "#                 print('generator', p[-1].shape, p[-1][:3])\n",
    "#                 break\n",
    "\n",
    "        print('2', round(float(loss), 2), round(float(generator_accuracy_cumulative), 2), round(float(discriminator_accuracy_cumulative), 2), end='\\r') \n",
    "    print()\n",
    "    if epoch%10==0:\n",
    "        generated_from_noise = generator.forward(*FIXED_NOISE)\n",
    "        for example in generated_from_noise:\n",
    "            print(message_recovery_with_metric(example))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7632)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator_accuracy_cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5945637226104736"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7d5932ec95442db1151cab0e057689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86112237 0.55061267 0.73683379 0.98912935 0.66783433 0.73186608\n",
      " 0.67646187 0.17029581 0.35461296 0.30057203 0.37228997 0.99491234\n",
      " 0.81527281 0.50484055 0.37183791 0.94053716 0.20716862 0.2022852\n",
      " 0.97283403 0.88754936 0.70003975 0.69724117 0.72908992 0.23675844\n",
      " 0.81361695 0.95489647 0.39190844 0.53617912 0.10331261 0.28917641\n",
      " 0.72324457 0.80936159 0.15441138 0.87727403 0.77709926 0.20245772\n",
      " 0.21236204 0.68839355 0.85525129 0.09353318 0.34596771 0.35584995\n",
      " 0.3328699  0.46926308 0.45900715 0.33370612 0.74128818 0.34074826\n",
      " 0.76966695 0.53535043 0.76732421 0.30665494 0.26330036 0.18248481\n",
      " 0.83154301 0.92831583 0.76089539 0.72926438 0.44008685 0.90272256\n",
      " 0.57554103 0.01934522 0.01958816 0.5142292  0.77750369 0.48777641\n",
      " 0.77452617 0.35590627 0.86501115 0.26052108 0.19019724 0.63555254\n",
      " 0.86174553 0.34463795 0.38288134 0.67530851 0.92020748 0.86416567\n",
      " 0.35036952 0.04390305 0.51494426 0.56524861 0.21908255 0.91570393\n",
      " 0.75533056 0.59345734 0.44710068 0.15626407 0.07728358 0.21268398\n",
      " 0.01249442 0.69255216 0.42780298 0.23715334 0.47515502 0.54124966\n",
      " 0.19248781 0.50765464 0.25925096 0.21059701 0.20572889 0.47057986\n",
      " 0.58655925 0.71745419 0.82399602 0.87501313 0.99431116 0.24931102\n",
      " 0.80010846 0.53984634 0.72671533 0.38220612 0.42336247 0.89794012\n",
      " 0.04941339 0.0923707  0.82983551 0.04304505 0.31402195 0.46315914\n",
      " 0.81994363 0.94835564 0.40436666 0.05257268 0.71781439 0.20198287\n",
      " 0.4618087  0.64586968 0.59464813 0.28396059 0.62163262 0.96392602\n",
      " 0.94110863 0.63046666 0.74121794 0.577046   0.68278188 0.91943484\n",
      " 0.63874974 0.84608641 0.78989976 0.77010462 0.68710956 0.79845225\n",
      " 0.28521996 0.00303837 0.89114945 0.47811315 0.33675825 0.8280472\n",
      " 0.35784838 0.08833064 0.974089   0.57078811 0.92839835 0.91145257\n",
      " 0.5202639  0.20777103 0.27977502 0.95212108 0.60432594 0.55030312\n",
      " 0.5175209  0.2725449  0.5509577  0.40275578 0.91250411 0.93451886\n",
      " 0.35960478 0.62109666 0.79052027 0.11202743 0.73972668 0.22469836\n",
      " 0.03429327 0.27110656 0.45603985 0.81483708 0.05154152 0.64041252\n",
      " 0.00361299 0.59179418 0.96200499 0.42646416 0.83325945 0.76203256\n",
      " 0.50565588 0.1140231  0.9847834  0.02287219 0.2028207  0.42400132\n",
      " 0.45129287 0.6349538  0.80116707 0.18057602 0.51608269 0.87274716\n",
      " 0.01325036 0.95099194]\n",
      "[-0.99833584  0.02226288 -0.70911545 -0.56301022 -0.24518657  0.33840847\n",
      "  1.88200688 -0.08435501  0.99568427  0.1722375   0.48785657  0.4074074\n",
      "  0.6538142  -0.22701219 -1.25403476 -1.54989266 -0.60041094 -1.29876709\n",
      " -2.54407263 -1.52077937  0.56938231 -1.38847923  0.94856429  0.21916068\n",
      "  0.53763741 -0.483953   -0.24481063  1.10400212 -0.35310096 -0.88286704\n",
      "  0.74152726  0.29003257  1.08094084  0.70721066  1.63394225  0.70530182\n",
      " -0.29778716 -0.84324068  1.37909079 -0.70705599  1.30713356  0.26339796\n",
      " -0.57403082 -1.44039404 -1.03593075 -0.21261756  0.88047528  0.20621392\n",
      "  0.40647823 -0.03818775 -0.05348476 -1.69562638 -1.55464637  0.20927456\n",
      " -0.85950965  0.24430278  1.92697787 -1.39320064 -1.67942941 -0.75467837\n",
      " -0.1933718   0.35632777 -0.05327565 -2.84187245  1.1999284  -0.18148756\n",
      " -0.49709737  0.32397431 -0.63812035 -0.26698539 -0.23300005 -0.45152104\n",
      "  0.10860274  1.94057691 -0.42762208 -0.2333844  -1.38103545 -1.07648134\n",
      " -0.1387444   0.05620682  0.19943775 -1.68176913 -0.09220861  0.06283973\n",
      "  0.15745552 -0.6695174   0.88557369  0.55961812 -1.71891034  0.25935498\n",
      "  0.39008722  0.09629015 -0.34884432  0.07141742 -1.49303508  0.78631037\n",
      " -0.40955159 -1.11830616 -0.55440098  0.26420224 -4.69007254  1.3989954\n",
      "  3.93796396  4.0497036  -1.14447463  3.33132792 -3.37408662  1.71937537\n",
      " -0.30640507  2.26373076 -0.802661    0.6772294   2.14062357  0.87663037\n",
      " -1.0238961   1.19073498 -2.99887466  1.9413842  -0.15227975 -5.27076197\n",
      "  1.45242131  0.57341498 -1.138394    1.97815418  0.04788882 -0.30670059\n",
      "  1.54309714  0.79103363 -1.77176905  1.92795408  0.71811008 -0.52918369\n",
      " -0.19921255  0.30805093  2.22163129  1.22239029 -3.22028112  4.22672653\n",
      " -4.5019846  -2.67574883 -0.09052869  0.08295289 -1.73235655  1.21253633\n",
      " -0.08308861 -1.01581621 -3.28605843  0.5611757  -2.48348427  2.05668592\n",
      "  2.35911894  1.64962578 -3.92566895  1.13029456  2.03636169 -1.330446\n",
      " -3.58890533 -2.58010459  2.44298029  0.42626819  1.3283118   5.00331688\n",
      " -4.07117224  2.31880498  0.43530071 -3.01922369 -2.7693131  -0.87138146\n",
      "  0.89397478 -2.93741798  1.11976337  0.37532836 -1.68834198 -2.09719586\n",
      " -4.33414125  0.36032486 -3.57536125 -0.66828835  2.26553845 -1.20059955\n",
      "  2.84474945 -2.3710556   1.20949888 -4.3074131   2.47549558 -2.12089825\n",
      "  2.55576038  5.12700224  0.6635946   2.20709038 -6.23049927  1.13088918\n",
      "  0.6639514  -1.99152565 -0.11131304  0.8929776  -6.1337924  -2.60050106\n",
      "  0.39461902  0.42088124]\n"
     ]
    }
   ],
   "source": [
    "for sent, lbl in data:\n",
    "    print(sent[0][0])\n",
    "    print(sent[0][-1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.4187e+01, -1.6576e+02,  1.7064e+01,  1.3220e+01,  7.1949e+01,\n",
       "         2.4682e+02,  4.0407e+02,  3.4279e+02,  2.0523e+02,  4.3445e+02,\n",
       "         2.2728e+02, -3.8771e+01,  1.4804e+02,  3.0545e+02,  4.9370e+01,\n",
       "        -2.1019e+02, -5.3297e-01,  1.8053e+02, -1.9329e+02, -2.3872e+02,\n",
       "        -3.2541e+02,  4.0822e+02,  1.8024e+02, -1.0698e+02,  1.2942e+02,\n",
       "        -2.1067e+02,  1.1527e+02, -7.9252e+01, -6.2347e-02, -2.4802e+02,\n",
       "        -8.3560e+01,  3.6910e+01,  2.1796e+02, -1.3657e+02,  1.1083e+02,\n",
       "        -4.4792e+01,  7.7739e+01,  2.2280e+00, -5.1198e+01,  2.7519e+02,\n",
       "        -6.0591e+00, -1.1030e+02,  7.9175e+01, -1.6940e+01, -1.8232e+02,\n",
       "         1.7351e+01, -7.7071e+01, -1.7007e+02,  2.2692e+02,  3.7427e+02,\n",
       "        -7.7924e+01,  1.4689e+02,  1.3271e+02, -3.8368e+02,  1.0483e+02,\n",
       "        -4.0391e+02,  3.3371e+02, -2.2014e+02,  7.0199e+01, -3.3544e+02,\n",
       "        -1.4662e+02, -2.5994e+02,  2.2845e+02,  9.6727e+01,  7.8687e+01,\n",
       "        -3.9376e+02,  2.4870e+02,  1.2706e+02,  1.2508e+01, -1.1350e+02,\n",
       "         1.8387e+02,  4.8248e+01,  1.6067e+02, -1.0784e+02,  9.9480e+01,\n",
       "        -2.2509e+02,  1.3881e+01, -3.5955e+02, -3.6180e+02, -2.3159e+02,\n",
       "        -1.8029e+02,  9.2165e+01,  2.6885e+02,  1.5836e+02,  6.7320e+01,\n",
       "         7.4141e+01,  4.3267e+01,  4.5468e+01, -1.7642e+01,  1.3922e+02,\n",
       "         3.5389e+01, -2.2086e+02,  8.8421e+01, -1.2686e+02, -4.7248e+01,\n",
       "         5.1306e+01, -6.7976e+01,  1.6073e+02,  3.3153e+02,  7.0814e+01,\n",
       "        -1.5836e+02,  1.5552e+02,  5.4475e+02,  1.4045e+02, -1.9903e+02,\n",
       "         6.0172e+02,  3.3791e+02,  4.9308e+01,  4.6916e+02,  4.2614e+02,\n",
       "         4.2647e+02,  4.8113e+02,  2.2174e+02, -1.7231e+02,  1.3771e+02,\n",
       "         3.9713e+02,  2.5792e+02,  3.3008e+02,  1.6589e+02, -2.7717e+02,\n",
       "         2.8899e+01, -4.9890e+00, -3.5793e+01,  2.6535e+02, -2.2832e+02,\n",
       "         4.0277e+02,  4.0036e+01,  1.3151e+02, -7.6126e+01,  1.5548e+02,\n",
       "         2.0587e+02,  3.8168e+02,  2.5064e+02, -4.2965e+02,  4.2087e+02,\n",
       "         5.2175e+02,  2.6873e+02,  4.7274e+02,  3.2530e+02,  4.5363e+02,\n",
       "        -2.1581e+02, -2.2858e+01,  3.2043e+02,  2.6081e+02,  2.9087e+01,\n",
       "         4.8084e+01, -2.0933e+02, -1.0834e+02,  3.3701e+02, -3.1498e+02,\n",
       "        -2.9382e+02,  3.6555e+02, -1.7623e+02,  1.2469e+02, -6.8858e+00,\n",
       "        -1.9832e+02, -3.4924e+02,  1.4054e+02,  2.1417e+01,  3.0111e+02,\n",
       "         2.3918e+02,  4.0890e+02, -1.5083e+02, -7.7367e+01,  2.7210e+02,\n",
       "        -2.8625e+02,  2.8412e+02,  2.7376e+02,  2.9112e+02,  9.3199e+01,\n",
       "         1.4695e+01, -2.7954e+02, -2.7732e+01,  1.4717e+02, -1.6059e+02,\n",
       "         2.6941e+02, -8.1506e+00, -2.2051e+02,  5.1874e+02,  3.7384e+02,\n",
       "         5.6975e+02,  1.3160e+01, -7.2322e+01,  3.6533e+02, -1.3402e+02,\n",
       "         2.0632e+02,  4.0106e+02,  5.6942e+02,  4.0327e+02,  3.6238e+02,\n",
       "        -4.5939e+02,  1.2699e+02,  3.1370e+02, -3.8268e+02, -2.4782e+02,\n",
       "         3.7322e+02,  1.7888e+02, -3.9703e+02, -2.4301e+01, -2.7633e+02],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_from_noise = generator.forward(*FIXED_NOISE)\n",
    "generated_from_noise[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-37131520.0000,   1662736.3750,  32547722.0000,  -4808051.5000,\n",
       "         27210840.0000,  40710744.0000,   5839760.0000,  38255852.0000,\n",
       "           802282.6875,  11119031.0000,  22401388.0000,  39451748.0000,\n",
       "         21651214.0000, -41066736.0000,  -1081529.8750,  -3389990.7500,\n",
       "        -37434056.0000, -14941159.0000,  -2824141.2500,  40229668.0000,\n",
       "          5393452.0000, -37853916.0000,  34104976.0000,  -1134469.5000,\n",
       "         31584468.0000, -17832026.0000,  -2870999.7500, -24167996.0000,\n",
       "         10778339.0000,  11420250.0000, -32118176.0000,  13808783.0000,\n",
       "         32759940.0000,  39977076.0000,  40493248.0000,  20001000.0000,\n",
       "         41552084.0000,  18724810.0000,  39620392.0000,  39530520.0000,\n",
       "          5711013.0000, -28776592.0000,  41093200.0000,   9103958.0000,\n",
       "        -28929074.0000,  -8888743.0000, -15586440.0000,  41048980.0000,\n",
       "         40715404.0000, -16867666.0000,  38182528.0000, -37584200.0000,\n",
       "         40531404.0000,  32121298.0000, -25105324.0000, -22692028.0000,\n",
       "         17229340.0000, -13385167.0000, -39766384.0000, -38643044.0000,\n",
       "        -26405334.0000,  20406294.0000,  -1728846.3750,  -7082606.5000,\n",
       "         39722776.0000,  40991168.0000, -11971971.0000,   7844889.0000,\n",
       "         26765348.0000,  40821784.0000,  12883452.0000,  37943792.0000,\n",
       "         11992098.0000,  -9854823.0000, -35535168.0000, -14503247.0000,\n",
       "        -39810232.0000, -27973872.0000, -20442468.0000,  34826672.0000,\n",
       "         -5868376.5000, -40483976.0000,  -6220495.0000,  35433032.0000,\n",
       "         30229484.0000,  31179644.0000, -26843104.0000,  18938640.0000,\n",
       "        -38484880.0000, -37368208.0000,   7651939.0000, -32424546.0000,\n",
       "        -15071656.0000,  14857478.0000,  41135384.0000,  38575996.0000,\n",
       "        -43357944.0000, -32389242.0000, -12663587.0000,  20199700.0000,\n",
       "        -41398448.0000,  37691452.0000,  44485996.0000,  41209484.0000,\n",
       "         11532534.0000,  39808936.0000, -14563251.0000, -39396480.0000,\n",
       "         -5729326.5000,  41302448.0000,   -927966.3750,  44088480.0000,\n",
       "         43438292.0000, -43437760.0000,  33692828.0000, -16955310.0000,\n",
       "         28274030.0000,  42952052.0000,  13583592.0000, -40022000.0000,\n",
       "         -4853736.5000,   2444270.5000,  28672116.0000,  33745192.0000,\n",
       "          3686073.7500,  40849828.0000,  -8213244.0000, -39876748.0000,\n",
       "        -44453708.0000,  43101312.0000, -39180964.0000,  25541878.0000,\n",
       "        -30102908.0000, -42953096.0000, -43120556.0000,   1302820.6250,\n",
       "         36255220.0000,  36657772.0000, -12095781.0000,  39999416.0000,\n",
       "        -41281440.0000, -14297016.0000,  30368652.0000, -36306144.0000,\n",
       "        -43253284.0000, -39638180.0000, -42795008.0000,  21733936.0000,\n",
       "        -28659310.0000,  40355548.0000, -40402244.0000,  29504464.0000,\n",
       "        -44361724.0000,  20174796.0000, -12037388.0000, -44303572.0000,\n",
       "        -43340008.0000,  39875016.0000,  42989252.0000,  31852292.0000,\n",
       "         42603384.0000,  42987100.0000, -39586456.0000,  39771384.0000,\n",
       "         44017856.0000, -34993384.0000,  41413392.0000, -28903132.0000,\n",
       "         41233200.0000, -32896926.0000,    715913.8125, -16826950.0000,\n",
       "         36668748.0000,  -1379697.7500, -29341590.0000,  40151468.0000,\n",
       "        -25564760.0000,  40478440.0000,  43900528.0000,  11900668.0000,\n",
       "         43819908.0000, -37004940.0000, -12710043.0000, -19631608.0000,\n",
       "         38674996.0000,  -2383038.0000,  44260544.0000,  44378044.0000,\n",
       "         35962128.0000,   6340772.5000, -37371688.0000,  43755488.0000,\n",
       "         43030672.0000, -44056500.0000,  36413228.0000,  43388704.0000,\n",
       "        -43797224.0000, -43769148.0000, -40202520.0000,  34945184.0000],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_from_noise = generator.forward(*FIXED_NOISE)\n",
    "generated_from_noise[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
