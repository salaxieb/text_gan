{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 13 09:02:36 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.51.05    Driver Version: 450.51.05    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:5E:00.0 Off |                    0 |\n",
      "| N/A   44C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  Off  | 00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    30W / 250W |    959MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d138cc924d30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mohe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from conllu import parse_incr\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "files = {'train': ['ru_syntagrus-ud-train-a.conllu', 'ru_syntagrus-ud-train-b.conllu', 'ru_syntagrus-ud-train-c.conllu'],\n",
    "         'test':  ['ru_syntagrus-ud-test.conllu'],\n",
    "         'dev':   ['ru_syntagrus-ud-dev.conllu']}\n",
    "\n",
    "labels = []\n",
    "sentences = []\n",
    "for data_type in files:\n",
    "    for filename in files[data_type]: \n",
    "        with open(os.path.join('UD_Russian-SynTagRus', filename), encoding='utf-8') as f:\n",
    "            parsed = parse_incr(f)\n",
    "            for token_list in parsed:\n",
    "                topic_name = token_list.metadata['sent_id'].split('.')[0]\n",
    "                # уберём цифры из названий темы\n",
    "                topic_name = re.sub(r'\\d+', '', topic_name)\n",
    "                sentence = ' '.join([token['form'] for token in token_list]).lower()\n",
    "                labels.append(topic_name)\n",
    "                sentences.append(sentence)\n",
    "\n",
    "le = LabelEncoder()\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(le.fit_transform(labels).reshape(-1, 1))\n",
    "\n",
    "labels = np.array(labels)\n",
    "sentences = np.array(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = -1\n",
    "lengths = []\n",
    "for message in sentences:\n",
    "    max_len = len(message.split()) if len(message.split()) > max_len else max_len\n",
    "    lengths.append(len(message.split()))\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(lengths, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LEN = 52\n",
    "EMBEDDING_SIZE = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!ЭМБЕДИНГИ НЕ КАЧЕСТВЕННЫЕ, не сдавайся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('word2vec_100.pkl', 'rb') as rf:\n",
    "    W2V_100 = pickle.load(rf)\n",
    "with open('fasttext_100.pkl', 'rb') as rf:\n",
    "    FT_100 = pickle.load(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('накрахмаленных', 0.81927490234375),\n",
       "  (';', 0.8096592426300049),\n",
       "  ('рядовыми', 0.8064565062522888),\n",
       "  ('чеченская', 0.7928946018218994),\n",
       "  ('соцстранах', 0.7922433614730835),\n",
       "  ('дымящих', 0.7861340641975403),\n",
       "  ('одиночного', 0.7822094559669495),\n",
       "  ('наркотикам', 0.7802185416221619),\n",
       "  ('линейный', 0.7785296440124512),\n",
       "  ('качественную', 0.7770684361457825)],\n",
       " [('он', 0.9201070070266724),\n",
       "  ('уж', 0.8810179829597473),\n",
       "  ('мыл', 0.8596895933151245),\n",
       "  ('мышь', 0.8507342338562012),\n",
       "  ('онф', 0.8464065194129944),\n",
       "  ('мне', 0.8424739241600037),\n",
       "  ('даешь', 0.838414192199707),\n",
       "  ('он-лайн', 0.8318077921867371),\n",
       "  ('ешь', 0.8273506760597229),\n",
       "  ('да', 0.8267568349838257)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2V_100.wv.most_similar(positive='.'), FT_100.wv.most_similar('я')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequence, target_len: int=20, embedding_size: int=300) -> np.array:\n",
    "    sequence = np.array(sequence)\n",
    "    if sequence.size == 0:\n",
    "        # empty array\n",
    "        current_length = 0\n",
    "        return np.zeros((target_len, embedding_size))\n",
    "    elif len(sequence.shape) == 1:\n",
    "        sequence = np.array([sequence])\n",
    "        current_length = 1\n",
    "    else:\n",
    "        current_length = sequence.shape[0]\n",
    "        \n",
    "    if current_length >= target_len:\n",
    "        return sequence[-target_len:]\n",
    "    \n",
    "    padding = np.random.uniform(size=(target_len - current_length, embedding_size))\n",
    "    #padding = np.zeros((target_len - current_length, embedding_size))\n",
    "    return np.concatenate((padding, sequence), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def vectorize_sentence(sentence):\n",
    "    vec = []\n",
    "    for token in sentence.split():\n",
    "        tkn_vec = np.concatenate((W2V_100.wv[token], FT_100.wv[token]), axis=0)\n",
    "        vec.append(tkn_vec)\n",
    "    return pad_sequences(vec, target_len=TARGET_LEN, embedding_size=EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['осенью россия уже пережила микрокризис ликвидности ( проще говоря , нехватку свободных денег ) .',\n",
       "        'жизнь ! . .',\n",
       "        'они купили примерно 75 % прироста внутреннего госдолга .'],\n",
       "       dtype='<U1218'),\n",
       " array(['Bankovskii_krizis', 'Baklanov', 'Dvadtsat_let_defoltu'],\n",
       "       dtype='<U53')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_DIM = 100\n",
    "\n",
    "\n",
    "class DataGenerator():\n",
    "    def __init__(self, labels, sentences, batches_per_epoch, batch_size):\n",
    "        self.train_sentences = sentences\n",
    "        self.train_labels = labels\n",
    "        self.batches_per_epoch = batches_per_epoch\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    @staticmethod\n",
    "    def rand_norm(npoints=1, ndim=RANDOM_DIM):\n",
    "        rand_vec = np.random.normal(0, 1, size=[npoints, ndim])\n",
    "        return rand_vec\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batches_per_epoch\n",
    "\n",
    "    def batch(self):\n",
    "        rand_sentences_indexes = np.random.randint(0, self.train_sentences.shape[0], size=self.batch_size)\n",
    "        sent_batch = np.array([vectorize_sentence(sent) for sent in self.train_sentences[rand_sentences_indexes]])\n",
    "        labels_batch = ohe.transform(le.transform(self.train_labels[rand_sentences_indexes]).reshape(-1, 1))\n",
    "        return sent_batch, labels_batch\n",
    "\n",
    "    def __iter__(self):\n",
    "        for b in tqdm(range(self.batches_per_epoch), leave=False):\n",
    "            yield self.batch()\n",
    "\n",
    "    def rand_batch(self):\n",
    "        rand_vec = self.rand_norm(self.batch_size)\n",
    "        # for training generator with label\n",
    "        rand_labels = np.random.randint(0, len(le.classes_), size=[self.batch_size])\n",
    "        rand_labels = ohe.transform(rand_labels.reshape(-1, 1)).todense()\n",
    "        return (rand_vec, rand_labels)\n",
    "\n",
    "    \n",
    "np.random.seed(42)\n",
    "sent_indexes = np.random.randint(0, len(sentences), size=3)\n",
    "MESSAGES_FROM_DATASET = [sentences[sent_indexes], labels[sent_indexes]]\n",
    "FIXED_NOISE = (DataGenerator.rand_norm(3), ohe.transform(le.transform(labels[sent_indexes]).reshape(-1, 1)).todense())\n",
    "MESSAGES_FROM_DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0998,  0.8525, -0.8415],\n",
       "         [ 0.0998,  0.8525,  0.8415]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def give_pe(batch_size, seq_len, features):\n",
    "    '''positiona encoding'''\n",
    "    # create constant 'pe' matrix with values dependant on \n",
    "    # seq_length and features\n",
    "    pe = torch.linspace(-1, 1, seq_len)\n",
    "    pe = pe.repeat(batch_size, features, 1)\n",
    "    pe = torch.transpose(pe, dim0=1, dim1=2)\n",
    "    position = torch.linspace(0.1, 1, features)\n",
    "    position = position.repeat(batch_size, seq_len, 1)\n",
    "    pe = torch.mul(pe, position)\n",
    "    pe[:, :, 0::2] = torch.sin(pe[:, :, 0::2])\n",
    "    pe[:, :, 1::2] = torch.cos(pe[:, :, 1::2])\n",
    "    return pe\n",
    "\n",
    "\n",
    "give_pe(1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainable params: 1801544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 52, 200])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, complexity=180):\n",
    "        super(Generator, self).__init__()\n",
    "        self.complexity = int(complexity)\n",
    "        # LAYER 1\n",
    "        self.fc_1 = torch.nn.Linear(\n",
    "            in_features=RANDOM_DIM + len(le.classes_),\n",
    "            out_features=TARGET_LEN//2//2*self.complexity,\n",
    "        )\n",
    "        self.init_small_std(self.fc_1)\n",
    "        self.activation_1 = torch.nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "        # LAYER 2\n",
    "        self.con1dT_2 = torch.nn.ConvTranspose1d(\n",
    "            in_channels=TARGET_LEN//2//2,\n",
    "            out_channels=TARGET_LEN//2,\n",
    "            kernel_size=3\n",
    "        )\n",
    "        self.init_small_std(self.con1dT_2)\n",
    "        self.batch_norm_2 = torch.nn.BatchNorm1d(\n",
    "            num_features=TARGET_LEN//2\n",
    "        )\n",
    "        self.activation_2 = torch.nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "        # LAYER 3\n",
    "        self.con1dT_3 = torch.nn.ConvTranspose1d(\n",
    "            in_channels=TARGET_LEN//2,\n",
    "            out_channels=TARGET_LEN,\n",
    "            kernel_size=3\n",
    "        )\n",
    "        self.init_small_std(self.con1dT_3)\n",
    "        self.batch_norm_3 = torch.nn.BatchNorm1d(\n",
    "            num_features=TARGET_LEN\n",
    "        )\n",
    "        self.activation_3 = torch.nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "        # LAYER 4\n",
    "        self.pe = give_pe(1, TARGET_LEN, self.complexity + 4)\n",
    "#         self.lstm_4 = torch.nn.LSTM(\n",
    "#             input_size=(self.complexity + 4)*2,\n",
    "#             hidden_size=self.complexity,\n",
    "#             bidirectional=True,\n",
    "#         )\n",
    "#         self.init_small_std(self.lstm_4)\n",
    "#         self.batch_norm_4 = torch.nn.BatchNorm1d(\n",
    "#             num_features=TARGET_LEN\n",
    "#         )\n",
    "#         self.activation_4 = torch.nn.Sigmoid()\n",
    "        \n",
    "        # LAYER 5\n",
    "        self.fc_5 = torch.nn.Linear(\n",
    "            #in_features=self.complexity * 2,\n",
    "            in_features=(self.complexity + 4)*2,\n",
    "            out_features=self.complexity * 2,\n",
    "        )\n",
    "        self.init_small_std(self.fc_5)\n",
    "        self.activation_5 = torch.nn.LeakyReLU()\n",
    "        \n",
    "        # LAYER 6\n",
    "        self.fc_6 = torch.nn.Linear(\n",
    "            in_features=self.complexity * 2,\n",
    "            out_features=EMBEDDING_SIZE,\n",
    "        )\n",
    "        self.init_small_std(self.fc_6)\n",
    "\n",
    "        #self.optimizer = torch.optim.SGD(self.parameters(), lr=0.01)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        self.is_real_loss = torch.nn.BCELoss()\n",
    "        self.category_loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        print(\n",
    "            'total trainable params:',\n",
    "            sum(p.numel() for p in self.parameters() if p.requires_grad),\n",
    "        )\n",
    "\n",
    "    def init_small_std(self, nn_module):\n",
    "        for p in nn_module.parameters():\n",
    "            torch.nn.init.normal_(p, std=0.02)\n",
    "\n",
    "    def forward(self, random_vec, labels):\n",
    "        random_vec_labels = torch.Tensor(random_vec), torch.Tensor(labels)\n",
    "        inp = torch.cat(random_vec_labels, 1)\n",
    "        \n",
    "        # LAYER 1\n",
    "        # (batch_size x (RANDOM_DIM + ohe_classes))\n",
    "        inp = self.fc_1(inp)\n",
    "        # (batch_size x (complexity * TARGET_LEN/2/2))\n",
    "        inp = self.activation_1(inp)\n",
    "        # (batch_size x (complexity * TARGET_LEN/2/2))\n",
    "        inp=torch.reshape(inp, (inp.shape[0], TARGET_LEN//2//2, self.complexity))\n",
    "        \n",
    "        # LAYER 2\n",
    "        # (batch_size x 13 x complexity)\n",
    "        inp = self.con1dT_2(inp)\n",
    "        # (batch_size x 26 x 66)\n",
    "        inp = self.batch_norm_2(inp)\n",
    "        # (batch_size x 26 x 66)\n",
    "        inp = self.activation_2(inp)\n",
    "        \n",
    "        # LAYER 3\n",
    "        # (batch_size x 26 x complexity + 2)\n",
    "        inp = self.con1dT_3(inp)\n",
    "        # (batch_size x 52 x complexity + 4)\n",
    "        inp = self.batch_norm_3(inp)\n",
    "        # (batch_size x 52 x complexity + 4)\n",
    "        inp = self.activation_3(inp)\n",
    "        \n",
    "        \n",
    "        # LAYER 4\n",
    "        # (batch_size x 52 x complexity + 4)\n",
    "        inp = torch.cat((inp, self.pe.repeat((len(inp), 1, 1))), 2)\n",
    "#         inp, _ = self.lstm_4(inp)\n",
    "#         # (batch_size x 52 x 2*complexity)\n",
    "#         inp = self.batch_norm_4(inp)\n",
    "#         # (batch_size x 52 x 2*complexity)\n",
    "#         inp = self.activation_4(inp)\n",
    "        \n",
    "        # LAYER 5\n",
    "        # (batch_size x 52 x 2*complexity)\n",
    "        inp = self.fc_5(inp)\n",
    "        inp = self.activation_5(inp)\n",
    "        \n",
    "        # LAYER 6\n",
    "        # (batch_size x 52 x 2*complexity)\n",
    "        inp = self.fc_6(inp)\n",
    "        \n",
    "        return inp\n",
    "\n",
    "generator = Generator()\n",
    "generated_from_noise = generator.forward(*FIXED_NOISE)\n",
    "generated_from_noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('переизбирается персидским вздоха упрека вздоха упрека отборочную напряжен отличиях бюджетниками', 0.9921572180509639)\n",
      "('долетает официанты вздоха поравнявшись гимны пожалел следователем слабел дневальный здравствующих', 1.0022923005476163)\n",
      "('предпочел полигамии упрека вздоха дневальный вздоха поравнявшись обнимался упрека мокро', 0.9932869984639603)\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "def message_recovery_with_metric(vector: np.ndarray):\n",
    "    assert len(vector.shape) == 2\n",
    "    tokens = []\n",
    "    metric = []\n",
    "    vector = vector.cpu().detach().numpy()\n",
    "    for token_vec in vector:\n",
    "        w2v = token_vec[:100]\n",
    "        token = W2V_100.wv.most_similar([w2v])[0][0]\n",
    "        tokens.append(token)\n",
    "        \n",
    "        metric.append(cosine(token_vec[100:], FT_100.wv[token]))\n",
    "    return ' '.join(tokens[-10:]), np.mean(metric)\n",
    "\n",
    "\n",
    "\n",
    "for example in generated_from_noise:\n",
    "    print(message_recovery_with_metric(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISCRIMINATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainable params: 2031760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.5156, 0.5148, 0.5156], grad_fn=<SigmoidBackward0>),\n",
       " tensor([[0.0016, 0.0016, 0.0016,  ..., 0.0018, 0.0019, 0.0017],\n",
       "         [0.0016, 0.0016, 0.0016,  ..., 0.0018, 0.0019, 0.0017],\n",
       "         [0.0016, 0.0016, 0.0016,  ..., 0.0018, 0.0019, 0.0017]],\n",
       "        grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, complexity=120):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.complexity = int(complexity)\n",
    "        # LAYER 1\n",
    "#         self.lstm_1 = torch.nn.LSTM(\n",
    "#             input_size=EMBEDDING_SIZE,\n",
    "#             hidden_size=self.complexity,\n",
    "#             bidirectional=True,\n",
    "#         )\n",
    "        #self.dropout_1 = torch.nn.Dropout2d(p=0.2)\n",
    "        #self.activation_1 = torch.nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "        # LAYER 2\n",
    "        self.conv1D_2 = torch.nn.ConvTranspose1d(\n",
    "            in_channels=TARGET_LEN,\n",
    "            out_channels=self.complexity,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "        )\n",
    "        self.dropout_2 = torch.nn.Dropout2d(p=0.2)\n",
    "        self.activation_2 = torch.nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "        # LAYER 3\n",
    "        self.conv1D_3 = torch.nn.ConvTranspose1d(\n",
    "            in_channels=self.complexity,\n",
    "            out_channels=self.complexity//2,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "        )\n",
    "        self.dropout_3 = torch.nn.Dropout2d(p=0.2)\n",
    "        self.activation_3 = torch.nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "        # LAYER 4\n",
    "        self.fc_4 = torch.nn.Linear(\n",
    "            in_features=803,#(self.complexity*4+1)*2+1,\n",
    "            out_features=self.complexity*2,\n",
    "        )\n",
    "        self.activation_4 = torch.nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.flatten_4 = torch.nn.Flatten(start_dim=1)\n",
    "        \n",
    "        # LAYER 5\n",
    "        self.fc_5 = torch.nn.Linear(\n",
    "            in_features=self.complexity*self.complexity,\n",
    "            out_features=self.complexity,\n",
    "        )\n",
    "        self.activation_5 = torch.nn.LeakyReLU(negative_slope=0.2)   \n",
    "        \n",
    "        \n",
    "        # LAYER 6_1\n",
    "        self.fc_6_1 = torch.nn.Linear(\n",
    "            in_features=self.complexity,\n",
    "            out_features=1,\n",
    "        )\n",
    "        self.flatten_6_1 = torch.nn.Flatten(start_dim=0)\n",
    "        self.activation_6_1 = torch.nn.Sigmoid()\n",
    "        \n",
    "        # LAYER 6_2\n",
    "        self.fc_6_2 = torch.nn.Linear(\n",
    "            in_features=self.complexity,\n",
    "            out_features=len(le.classes_),\n",
    "        )\n",
    "        self.activation_6_2 = torch.nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.is_real_loss = torch.nn.BCELoss()\n",
    "        self.category_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        \n",
    "        print(\n",
    "            'total trainable params:',\n",
    "            sum(p.numel() for p in self.parameters() if p.requires_grad),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, sentence_vec):\n",
    "        inp = sentence_vec\n",
    "        \n",
    "#         # LAYER 1\n",
    "#         # (batch_size x target_len x embedding_size)\n",
    "#         inp, _ = self.lstm_1(inp)\n",
    "#         # (batch_size x target_len x complexity*2)\n",
    "#         inp = self.dropout_1(inp)\n",
    "#         # (batch_size x target_len x complexity*2)\n",
    "#         inp = self.activation_1(inp)\n",
    "\n",
    "        # LAYER 2\n",
    "        # (batch_size x target_len x complexity*2)\n",
    "        inp = self.conv1D_2(inp)\n",
    "        # (batch_size x target_len/2 x complexity*2)\n",
    "        inp = self.dropout_2(inp)\n",
    "        # (batch_size x target_len/2 x complexity*2)\n",
    "        inp = self.activation_2(inp)\n",
    "        \n",
    "        # LAYER 3\n",
    "        # (batch_size x target_len x complexity*2)\n",
    "        inp = self.conv1D_3(inp)\n",
    "        # (batch_size x target_len/4 x complexity)\n",
    "        inp = self.dropout_3(inp)\n",
    "        # (batch_size x target_len/4 x complexity)\n",
    "        inp = self.activation_3(inp)\n",
    "        \n",
    "        \n",
    "        # LAYER 4\n",
    "        # (batch_size x target_len/4 x complexity)\n",
    "        inp = self.fc_4(inp)\n",
    "        # (batch_size x target_len/4 x complexity)\n",
    "        inp = self.activation_4(inp)\n",
    "        # (batch_size x target_len/4 x complexity)\n",
    "        inp = self.flatten_4(inp)\n",
    "        # (batch_size x target_len/4  complexity)\n",
    "        \n",
    "        # LAYER 5\n",
    "        # (batch_size x complexity * 2)\n",
    "        inp = self.fc_5(inp)\n",
    "        # (batch_size x complexity)\n",
    "        inp = self.activation_5(inp)\n",
    "        \n",
    "        # LAYER 6_1\n",
    "        # (batch_size x complexity)\n",
    "        inp_1 = self.fc_6_1(inp)\n",
    "        inp_1 = self.flatten_6_1(inp_1)\n",
    "        # (batch_size x 2)\n",
    "        inp_1 = self.activation_6_1(inp_1)\n",
    "        \n",
    "        # LAYER 6_2\n",
    "        # (batch_size x complexity)\n",
    "        inp_2 = self.fc_6_2(inp)\n",
    "        # (batch_size x 2)\n",
    "        inp_2 = self.activation_6_2(inp_2)\n",
    "        return inp_1, inp_2\n",
    "    \n",
    "    def train_on_batch(self, messages, y_real_fake, y_label):\n",
    "        messages = torch.Tensor(messages)\n",
    "        pred_real_fake, pred_label = self.forward(messages)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.is_real_loss(y_real_fake, pred_real_fake) + 0.01 * self.category_loss(y_label, pred_label)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        accuracy = torch.mean(((pred_real_fake > 0.5).int() == (y_real_fake > 0.5).int()).float())\n",
    "        return loss, accuracy\n",
    "\n",
    "\n",
    "discriminator = Discriminator()\n",
    "discriminator.forward(generated_from_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(torch.nn.Module):\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super(GAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        \n",
    "    def train_generator_on_batch(self, noise, y_real_fake, y_label):\n",
    "        sentence = self.generator.forward(*noise)\n",
    "        pred_real_fake, pred_label = self.discriminator.forward(sentence)\n",
    "        loss_1 = self.discriminator.is_real_loss(y_real_fake, pred_real_fake)\n",
    "        loss_2 = self.discriminator.category_loss(torch.Tensor(y_label), pred_label)\n",
    "        loss = loss_1 + 0.01 * loss_2\n",
    "        # self.generator.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.generator.optimizer.step()\n",
    "        accuracy = torch.mean(((pred_real_fake > 0.5).int() == (y_real_fake > 0.5).int()).float()).numpy()\n",
    "        return loss, accuracy\n",
    "\n",
    "gan = GAN(generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d8ba9f12244853b655adb6b2929eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(3.3794, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t99999999999 \t\tsor(1.6918, grad_fn=<AddBackward0>) \t 0.4881504342551754 \t 0.5118495657448245 \ttensor(2.3725, grad_fn=<AddBackward0>) \t 0.31545967199240804 \t 0.6845403280075919 \ttensor(3.0932, grad_fn=<AddBackward0>) \t 0.14318649132567465 \t 0.8568135086743254 \ttensor(3.6282, grad_fn=<AddBackward0>) \t 0.009032186437837326 \t 0.9909678135621627 \ttensor(3.6007, grad_fn=<AddBackward0>) \t 3.3934846869258704e-05 \t 0.9999660651531307 \ttensor(3.5802, grad_fn=<AddBackward0>) \t 9.584203071838452e-06 \t 0.9999904157969282 \ttensor(3.4119, grad_fn=<AddBackward0>) \t 4.864258950368348e-09 \t 0.999999995135741 \ttensor(3.6805, grad_fn=<AddBackward0>) \t 4.561117094648163e-150 \t 1.0 \t \t 5.6813504046155626e-273 \t 1.0 \ttensor(3.3695, grad_fn=<AddBackward0>) \t 1.7218585972169465e-289 \t 1.0 \t 2.5e-323 \t 1.0 \ttensor(3.5660, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.7247, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.6272, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3614, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2375, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1905, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t \t tensor(3.4985, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4821, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.4377, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3713, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4929, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.6473, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3610, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4671, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.4904, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t \t 2.5e-323 \t 1.0 \ttensor(3.4582, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4521, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2917, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5452, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t \t 2.5e-323 \t 1.0 \ttensor(3.5258, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3242, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.6365, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1425, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5635, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3421, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2842, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2501, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(2.9211, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3133, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4448, grad_fn=<AddBackward0>) \t 4.757472975558678e-58 \t 1.0 \t tensor(3.3873, grad_fn=<AddBackward0>) \t 5.35676454002388e-64 \t 1.0 \ttensor(3.4120, grad_fn=<AddBackward0>) \t 1.3408814647313375e-65 \t 1.0 \ttensor(3.5231, grad_fn=<AddBackward0>) \t 2.3011587312778112e-71 \t 1.0 \ttensor(3.2638, grad_fn=<AddBackward0>) \t 1.2043059752344874e-72 \t 1.0 \ttensor(3.7727, grad_fn=<AddBackward0>) \t 7.111306353162126e-73 \t 1.0 \ttensor(3.5074, grad_fn=<AddBackward0>) \t 1.4198989736088117e-75 \t 1.0 \ttensor(3.5854, grad_fn=<AddBackward0>) \t 3.1922120253185267e-84 \t 1.0 \ttensor(3.0034, grad_fn=<AddBackward0>) \t 7.302757197544269e-85 \t 1.0 \t \t 2.062514330335715e-85 \t 1.0 \ttensor(3.4430, grad_fn=<AddBackward0>) \t 2.3947092210268436e-89 \t 1.0 \ttensor(3.4410, grad_fn=<AddBackward0>) \t 1.910174367646995e-90 \t 1.0 \ttensor(3.0608, grad_fn=<AddBackward0>) \t 6.660366188301581e-91 \t 1.0 \ttensor(3.5144, grad_fn=<AddBackward0>) \t 1.7690864306487922e-95 \t 1.0 \ttensor(3.4985, grad_fn=<AddBackward0>) \t 1.2316355124500139e-98 \t 1.0 \t1.3069089021855003e-101 \t 1.0 \ttensor(3.1469, grad_fn=<AddBackward0>) \t 3.3219870792042915e-102 \t 1.0 \t \t 1.9616001503993425e-102 \t 1.0 \ttensor(3.2601, grad_fn=<AddBackward0>) \t 1.5888961218234675e-102 \t 1.0 \ttensor(3.1877, grad_fn=<AddBackward0>) \t 3.977254074204827e-104 \t 1.0 \t1.6860040696036549e-105 \t 1.0 \ttensor(3.1335, grad_fn=<AddBackward0>) \t 4.7617734788324e-106 \t 1.0 \ttensor(3.2919, grad_fn=<AddBackward0>) \t 2.5306016593641694e-106 \t 1.0 \ttensor(3.3570, grad_fn=<AddBackward0>) \t 1.6603277487088319e-106 \t 1.0 \t tensor(3.6054, grad_fn=<AddBackward0>) \t 4.030436395695474e-110 \t 1.0 \ttensor(3.5274, grad_fn=<AddBackward0>) \t 2.1705803402732138e-116 \t 1.0 \ttensor(3.2395, grad_fn=<AddBackward0>) \t 7.453086595041153e-118 \t 1.0 \ttensor(3.4008, grad_fn=<AddBackward0>) \t 2.3388575471002534e-118 \t 1.0 \ttensor(3.4980, grad_fn=<AddBackward0>) \t 9.914688046231084e-120 \t 1.0 \ttensor(3.3699, grad_fn=<AddBackward0>) \t 4.669945669079738e-121 \t 1.0 \ttensor(3.5488, grad_fn=<AddBackward0>) \t 1.4431605197007787e-122 \t 1.0 \ttensor(3.4427, grad_fn=<AddBackward0>) \t 3.6683204098209315e-123 \t 1.0 \ttensor(3.1687, grad_fn=<AddBackward0>) \t 1.5080403456145396e-126 \t 1.0 \ttensor(3.4421, grad_fn=<AddBackward0>) \t 9.74357216527324e-128 \t 1.0 \ttensor(3.3384, grad_fn=<AddBackward0>) \t 1.2764279363898623e-130 \t 1.0 \t tensor(3.2519, grad_fn=<AddBackward0>) \t 2.293754148791109e-133 \t 1.0 \t\t 3.7670790633012636e-135 \t 1.0 \ttensor(3.3364, grad_fn=<AddBackward0>) \t 1.6945060820915593e-139 \t 1.0 \ttensor(3.6061, grad_fn=<AddBackward0>) \t 2.254166643864989e-141 \t 1.0 \ttensor(3.3083, grad_fn=<AddBackward0>) \t 3.989071246171949e-145 \t 1.0 \ttensor(3.5521, grad_fn=<AddBackward0>) \t 1.9079604109231807e-145 \t 1.0 \ttensor(3.4057, grad_fn=<AddBackward0>) \t 1.013968388741426e-145 \t 1.0 \ttensor(3.6924, grad_fn=<AddBackward0>) \t 8.986739084699017e-147 \t 1.0 \ttensor(3.2480, grad_fn=<AddBackward0>) \t 8.582380317406911e-150 \t 1.0 \ttensor(3.2981, grad_fn=<AddBackward0>) \t 3.801730790800483e-155 \t 1.0 \t \t 8.305812356635464e-159 \t 1.0 \t \t 1.5156449404561713e-160 \t 1.0 \ttensor(3.3075, grad_fn=<AddBackward0>) \t 4.602983705060344e-170 \t 1.0 \ttensor(3.4585, grad_fn=<AddBackward0>) \t 2.796955221836176e-175 \t 1.0 \ttensor(3.7105, grad_fn=<AddBackward0>) \t 7.001213860758826e-177 \t 1.0 \ttensor(3.2339, grad_fn=<AddBackward0>) \t 7.883146018718569e-183 \t 1.0 \ttensor(3.1101, grad_fn=<AddBackward0>) \t 8.625658685442922e-184 \t 1.0 \ttensor(3.8609, grad_fn=<AddBackward0>) \t 1.1651985851291542e-184 \t 1.0 \t1.0 \ttensor(3.4509, grad_fn=<AddBackward0>) \t 1.8275254756849585e-189 \t 1.0 \ttensor(3.3678, grad_fn=<AddBackward0>) \t 5.561621878714113e-192 \t 1.0 \ttensor(3.2616, grad_fn=<AddBackward0>) \t 5.799675893649862e-203 \t 1.0 \ttensor(3.3898, grad_fn=<AddBackward0>) \t 7.834502142883566e-204 \t 1.0 \ttensor(3.5916, grad_fn=<AddBackward0>) \t 3.976235361934088e-207 \t 1.0 \ttensor(3.3999, grad_fn=<AddBackward0>) \t 8.04542289022299e-216 \t 1.0 \ttensor(3.3740, grad_fn=<AddBackward0>) \t 1.8125050137894304e-217 \t 1.0 \ttensor(2.8709, grad_fn=<AddBackward0>) \t 4.399845862142809e-221 \t 1.0 \ttensor(3.4309, grad_fn=<AddBackward0>) \t 8.785075924785034e-224 \t 1.0 \ttensor(3.4067, grad_fn=<AddBackward0>) \t 1.0200027226515969e-227 \t 1.0 \ttensor(3.1029, grad_fn=<AddBackward0>) \t 1.8329553985456855e-230 \t 1.0 \ttensor(3.5085, grad_fn=<AddBackward0>) \t 9.283602166683284e-241 \t 1.0 \t tensor(3.4798, grad_fn=<AddBackward0>) \t 8.865886030410681e-244 \t 1.0 \t \t 7.857779926342146e-245 \t 1.0 \ttensor(3.2874, grad_fn=<AddBackward0>) \t 1.3665476737966292e-256 \t 1.0 \t \t 4.764857112216924e-257 \t 1.0 \ttensor(3.1647, grad_fn=<AddBackward0>) \t 3.0786131992497694e-258 \t 1.0 \t \t 8.303684950940863e-262 \t 1.0 \ttensor(3.4076, grad_fn=<AddBackward0>) \t 5.2833735345131105e-264 \t 1.0 \t 5.4368186289771805e-269 \t 1.0 \ttensor(3.3671, grad_fn=<AddBackward0>) \t 2.2649534481746838e-278 \t 1.0 \ttensor(3.3737, grad_fn=<AddBackward0>) \t 4.070144675196618e-281 \t 1.0 \ttensor(3.2786, grad_fn=<AddBackward0>) \t 3.9471193824402566e-283 \t 1.0 \ttensor(3.3985, grad_fn=<AddBackward0>) \t 1.5291949212863824e-283 \t 1.0 \ttensor(3.2550, grad_fn=<AddBackward0>) \t 1.2386478862419697e-283 \t 1.0 \ttensor(3.3755, grad_fn=<AddBackward0>) \t 8.126768781633563e-284 \t 1.0 \ttensor(3.4687, grad_fn=<AddBackward0>) \t 8.75680763609511e-286 \t 1.0 \t tensor(3.5899, grad_fn=<AddBackward0>) \t 9.581610395606356e-287 \t 1.0 \ttensor(3.5795, grad_fn=<AddBackward0>) \t 5.092060610251439e-287 \t 1.0 \ttensor(3.4838, grad_fn=<AddBackward0>) \t 1.1649001374880427e-287 \t 1.0 \ttensor(3.3887, grad_fn=<AddBackward0>) \t 1.4768661984181794e-299 \t 1.0 \ttensor(3.1993, grad_fn=<AddBackward0>) \t 1.5913622684856893e-301 \t 1.0 \ttensor(3.4462, grad_fn=<AddBackward0>) \t 4.983570480583864e-309 \t 1.0 \ttensor(3.1438, grad_fn=<AddBackward0>) \t 2.37871334e-316 \t 1.0 \ttensor(3.4542, grad_fn=<AddBackward0>) \t 6.4164e-320 \t 1.0 \ttensor(3.4497, grad_fn=<AddBackward0>) \t 4.4e-323 \t 1.0 \t tensor(3.2604, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t\ttensor(3.5629, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.4903, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1358, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t\t \t 2.5e-323 \t 1.0 \ttensor(3.4783, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.7288, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4271, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1988, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.3731, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4931, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2893, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3780, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2579, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5041, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1794, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(2.9567, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4917, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.7386, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1788, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2389, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3137, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5661, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5585, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5803, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.6403, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2557, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.2821, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.0427, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.3422, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5160, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4010, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5073, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4015, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.6278, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3157, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3581, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2971, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1911, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2224, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3087, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2217, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2421, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2842, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.6424, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2255, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3549, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4769, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3161, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3045, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5975, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3356, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4209, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3407, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5704, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t \t 2.5e-323 \t 1.0 \ttensor(3.1457, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1538, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4029, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2301, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3815, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5888, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t\ttensor(3.4555, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.7199, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4730, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t \t 2.5e-323 \t 1.0 \ttensor(3.3993, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2035, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2565, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.6666, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1665, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5914, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4380, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1814, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t \t 2.5e-323 \t 1.0 \ttensor(3.2714, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2033, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2776, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3719, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4770, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2927, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t\t 2.5e-323 \t 1.0 \ttensor(3.2987, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2794, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5769, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3632, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2814, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3931, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2255, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.0934, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.8050, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5789, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4623, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4363, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.0739, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.5119, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5953, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1276, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2801, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2931, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.0997, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t \t 2.5e-323 \t 1.0 \ttensor(3.6495, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2466, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.7321, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5619, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t \t 2.5e-323 \t 1.0 \ttensor(3.3471, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5786, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t \t 2.5e-323 \t 1.0 \ttensor(3.7053, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2349, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1073, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4153, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4329, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2335, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t\ttensor(3.4975, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2289, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1781, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.2922, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.4459, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4258, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4381, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3133, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.7792, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1245, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5049, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t \t 2.5e-323 \t 1.0 \t\ttensor(3.4645, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4317, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t\ttensor(3.4723, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3536, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1539, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4508, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1872, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2296, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.3647, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2586, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1396, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.9337, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.0164, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2472, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5256, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1826, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.3450, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.2356, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5103, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t \t 2.5e-323 \t 1.0 \ttensor(3.1677, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.0066, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.6190, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t \t 2.5e-323 \t 1.0 \ttensor(3.1489, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5169, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.6037, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4459, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3865, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.6437, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t 2.5e-323 \t 1.0 \ttensor(3.5139, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2057, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3742, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5657, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t \t 2.5e-323 \t 1.0 \ttensor(3.3978, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.6985, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2619, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t \t 1.0 \t \t 2.5e-323 \t 1.0 \ttensor(3.4624, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2988, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2126, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2969, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t \t 2.5e-323 \t 1.0 \ttensor(3.2143, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4354, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2209, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5223, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(2.9914, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3157, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2055, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t\t 2.5e-323 \t 1.0 \ttensor(3.4114, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5177, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2323, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5943, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3041, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.5485, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4526, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.0055, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3106, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5337, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4759, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3320, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.5923, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t \t 2.5e-323 \t 1.0 \t \t 2.5e-323 \t 1.0 \ttensor(3.4784, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5073, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4057, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2134, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.6384, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2167, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3116, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2501, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4372, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4965, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2942, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4503, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4015, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.6667, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t \t 2.5e-323 \t 1.0 \ttensor(3.7273, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3240, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1385, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t \t \t 2.5e-323 \t 1.0 \ttensor(3.6735, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.6912, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3128, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4252, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2532, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3355, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.6152, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5070, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3398, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3378, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3657, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3290, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5821, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t\t 1.0 \ttensor(3.4846, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.2049, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1908, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2655, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3040, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4916, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2677, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.4613, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2563, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t \t 2.5e-323 \t 1.0 \ttensor(3.1715, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2181, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2162, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.6713, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5402, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1996, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3397, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4433, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.1861, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.4855, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5729, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.0111, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3957, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1641, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2143, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(2.9615, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t tensor(3.2513, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.1988, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3639, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5332, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4935, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5414, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.2897, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.3657, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.4014, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t \t 1.0 \ttensor(3.2204, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t \t 2.5e-323 \t 1.0 \ttensor(3.2433, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \ttensor(3.5826, grad_fn=<AddBackward0>) \t 2.5e-323 \t 1.0 \t\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-748c249f5a84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# train GAN and save accuracy to array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_generator_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_real_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mgenerator_accuracy_cumulative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgenerator_accuracy_cumulative\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-677f559c90c1>\u001b[0m in \u001b[0;36mtrain_generator_on_batch\u001b[0;34m(self, noise, y_real_fake, y_label)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.01\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# self.generator.optimizer.zero_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_real_fake\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_real_fake\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generator_acc_follow = []\n",
    "generator_accuracy_cumulative = 0.5\n",
    "discriminator_accuracy_cumulative = 0.5\n",
    "loss_cumulative = 1\n",
    "BATCH_SIZE = 32\n",
    "data = DataGenerator(labels, sentences, batches_per_epoch=2000, batch_size=BATCH_SIZE)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for sent_batch, labels_batch in data:\n",
    "        ##########################\n",
    "        # discriminator training #\n",
    "        ##########################\n",
    "        noise = data.rand_batch()\n",
    "        generated_texts = generator.forward(*noise)\n",
    "        # X\n",
    "        X = torch.cat((torch.Tensor(sent_batch), generated_texts), 0)\n",
    "        # target valuest smoothing\n",
    "        y_real = np.random.uniform(0.8, 1, size=[BATCH_SIZE])\n",
    "        y_fake = np.random.uniform(0, 0.2, size=[BATCH_SIZE])\n",
    "        # Y\n",
    "        y_real_fake = torch.Tensor(np.concatenate((y_real, y_fake)))\n",
    "        y_labels = torch.cat((\n",
    "            torch.Tensor(labels_batch.todense()),\n",
    "            torch.Tensor(np.zeros((BATCH_SIZE, len(le.classes_)))),\n",
    "        ), 0)        \n",
    "        \n",
    "        loss, accuracy = discriminator.train_on_batch(X, y_real_fake, y_labels)\n",
    "        discriminator_accuracy_cumulative = 0.9 * discriminator_accuracy_cumulative + 0.1 * accuracy\n",
    "        generator_accuracy_cumulative = 1 - generator_accuracy_cumulative\n",
    "        \n",
    "        ######################\n",
    "        # generator training #\n",
    "        ######################\n",
    "        while discriminator_accuracy_cumulative > 0.5:\n",
    "            noise = data.rand_batch()\n",
    "            # target values\n",
    "            y_real_fake = torch.Tensor(np.random.uniform(0.9, 1.0, size=[BATCH_SIZE]))\n",
    "\n",
    "            # train GAN and save accuracy to array\n",
    "            loss, accuracy = gan.train_generator_on_batch(noise, y_real_fake, noise[1])\n",
    "\n",
    "            generator_accuracy_cumulative = 0.9 * generator_accuracy_cumulative + 0.1 * accuracy\n",
    "            discriminator_accuracy_cumulative = 1 - generator_accuracy_cumulative\n",
    "            print('1', loss, '\\t', generator_accuracy_cumulative, '\\t', discriminator_accuracy_cumulative, '\\t', end='\\r')\n",
    "        print('2', loss, '\\t', generator_accuracy_cumulative, '\\t', discriminator_accuracy_cumulative, '\\t', end='\\r') \n",
    "#         for p in discriminator.parameters():\n",
    "#             print(p[-1].shape, p[-1][:10])\n",
    "#             break\n",
    "#     break\n",
    "    print()\n",
    "    if epoch%10==0:\n",
    "        generated_from_noise = generator.forward(*FIXED_NOISE)\n",
    "        for example in generated_from_noise:\n",
    "            print(message_recovery_with_metric(example))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9777f7913fd49cf9690bd04baea5490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06458079 0.88444217 0.04627904 0.01625815 0.47432459 0.76541899\n",
      " 0.12418893 0.24392195 0.28025692 0.76642438 0.6068362  0.07439028\n",
      " 0.06757367 0.41487956 0.76637193 0.58697913 0.65224034 0.97039418\n",
      " 0.5478836  0.11985323 0.72954896 0.64719638 0.95548785 0.3016512\n",
      " 0.22289601 0.06568971 0.9324067  0.61725998 0.17226599 0.45509105\n",
      " 0.74441921 0.11003214 0.6409427  0.92842726 0.40769911 0.01004235\n",
      " 0.07875389 0.33903056 0.67638938 0.07533897 0.68827802 0.03807154\n",
      " 0.73976101 0.27656204 0.50372155 0.30466523 0.67049912 0.43468589\n",
      " 0.88435819 0.31126294 0.69641774 0.87996534 0.03831658 0.78288389\n",
      " 0.32955603 0.14168164 0.63375975 0.97035296 0.67075908 0.52466526\n",
      " 0.04520703 0.96242532 0.47869339 0.514479   0.69143258 0.35431984\n",
      " 0.37044873 0.61572151 0.44142991 0.06722276 0.84994126 0.22030675\n",
      " 0.29160039 0.66756081 0.33088835 0.53921697 0.91767782 0.75352843\n",
      " 0.02509498 0.53808318 0.87084049 0.08328986 0.54560594 0.95591056\n",
      " 0.4870287  0.09907154 0.15488052 0.91138416 0.93317906 0.01248402\n",
      " 0.05696831 0.82554366 0.59148879 0.09861804 0.42302186 0.28171651\n",
      " 0.39079132 0.68138948 0.7252524  0.85341733 0.91490848 0.32696104\n",
      " 0.28520787 0.17560929 0.77550288 0.21508473 0.73692857 0.12731545\n",
      " 0.96158499 0.35526428 0.21900664 0.69425121 0.63074767 0.24364566\n",
      " 0.27049788 0.54143107 0.94636533 0.39423201 0.8080253  0.62662734\n",
      " 0.64384177 0.54005921 0.06368545 0.01457027 0.97233463 0.5198881\n",
      " 0.63183089 0.45306949 0.85742283 0.862376   0.89076641 0.90002999\n",
      " 0.49520032 0.02004986 0.29272844 0.77865509 0.80145577 0.06303868\n",
      " 0.13563609 0.51419163 0.4212245  0.07306283 0.98077543 0.10172922\n",
      " 0.08375216 0.06791075 0.06995318 0.80179781 0.43023393 0.24800055\n",
      " 0.28294125 0.11547543 0.08365759 0.44968639 0.53010396 0.12082991\n",
      " 0.55045261 0.05392694 0.57470113 0.01961649 0.93541577 0.44857568\n",
      " 0.31226179 0.62909385 0.82125374 0.19925121 0.57521791 0.9369616\n",
      " 0.81752617 0.33098307 0.95354264 0.51810066 0.66724098 0.09155246\n",
      " 0.75230272 0.00731976 0.37565409 0.95039752 0.07212359 0.65080799\n",
      " 0.74750768 0.70098332 0.4038337  0.35223285 0.45723292 0.19675466\n",
      " 0.99918387 0.48790415 0.91683427 0.93846715 0.13163271 0.62295983\n",
      " 0.32778481 0.26211291 0.51423919 0.35437469 0.65374822 0.32305101\n",
      " 0.22715049 0.89035882]\n",
      "[-0.99833584  0.02226288 -0.70911545 -0.56301022 -0.24518657  0.33840847\n",
      "  1.88200688 -0.08435501  0.99568427  0.1722375   0.48785657  0.4074074\n",
      "  0.6538142  -0.22701219 -1.25403476 -1.54989266 -0.60041094 -1.29876709\n",
      " -2.54407263 -1.52077937  0.56938231 -1.38847923  0.94856429  0.21916068\n",
      "  0.53763741 -0.483953   -0.24481063  1.10400212 -0.35310096 -0.88286704\n",
      "  0.74152726  0.29003257  1.08094084  0.70721066  1.63394225  0.70530182\n",
      " -0.29778716 -0.84324068  1.37909079 -0.70705599  1.30713356  0.26339796\n",
      " -0.57403082 -1.44039404 -1.03593075 -0.21261756  0.88047528  0.20621392\n",
      "  0.40647823 -0.03818775 -0.05348476 -1.69562638 -1.55464637  0.20927456\n",
      " -0.85950965  0.24430278  1.92697787 -1.39320064 -1.67942941 -0.75467837\n",
      " -0.1933718   0.35632777 -0.05327565 -2.84187245  1.1999284  -0.18148756\n",
      " -0.49709737  0.32397431 -0.63812035 -0.26698539 -0.23300005 -0.45152104\n",
      "  0.10860274  1.94057691 -0.42762208 -0.2333844  -1.38103545 -1.07648134\n",
      " -0.1387444   0.05620682  0.19943775 -1.68176913 -0.09220861  0.06283973\n",
      "  0.15745552 -0.6695174   0.88557369  0.55961812 -1.71891034  0.25935498\n",
      "  0.39008722  0.09629015 -0.34884432  0.07141742 -1.49303508  0.78631037\n",
      " -0.40955159 -1.11830616 -0.55440098  0.26420224 -4.69007254  1.3989954\n",
      "  3.93796396  4.0497036  -1.14447463  3.33132792 -3.37408662  1.71937537\n",
      " -0.30640507  2.26373076 -0.802661    0.6772294   2.14062357  0.87663037\n",
      " -1.0238961   1.19073498 -2.99887466  1.9413842  -0.15227975 -5.27076197\n",
      "  1.45242131  0.57341498 -1.138394    1.97815418  0.04788882 -0.30670059\n",
      "  1.54309714  0.79103363 -1.77176905  1.92795408  0.71811008 -0.52918369\n",
      " -0.19921255  0.30805093  2.22163129  1.22239029 -3.22028112  4.22672653\n",
      " -4.5019846  -2.67574883 -0.09052869  0.08295289 -1.73235655  1.21253633\n",
      " -0.08308861 -1.01581621 -3.28605843  0.5611757  -2.48348427  2.05668592\n",
      "  2.35911894  1.64962578 -3.92566895  1.13029456  2.03636169 -1.330446\n",
      " -3.58890533 -2.58010459  2.44298029  0.42626819  1.3283118   5.00331688\n",
      " -4.07117224  2.31880498  0.43530071 -3.01922369 -2.7693131  -0.87138146\n",
      "  0.89397478 -2.93741798  1.11976337  0.37532836 -1.68834198 -2.09719586\n",
      " -4.33414125  0.36032486 -3.57536125 -0.66828835  2.26553845 -1.20059955\n",
      "  2.84474945 -2.3710556   1.20949888 -4.3074131   2.47549558 -2.12089825\n",
      "  2.55576038  5.12700224  0.6635946   2.20709038 -6.23049927  1.13088918\n",
      "  0.6639514  -1.99152565 -0.11131304  0.8929776  -6.1337924  -2.60050106\n",
      "  0.39461902  0.42088124]\n"
     ]
    }
   ],
   "source": [
    "for sent, lbl in data:\n",
    "    print(sent[0][0])\n",
    "    print(sent[0][-1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88826b7908ec4b76933428b613d84b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = DataGenerator(labels, sentences, batches_per_epoch=20, batch_size=3)\n",
    "for sentence, label in data:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.99833584,  0.02226288, -0.70911545, -0.56301022, -0.24518657,\n",
       "        0.33840847,  1.88200688, -0.08435501,  0.99568427,  0.1722375 ,\n",
       "        0.48785657,  0.4074074 ,  0.6538142 , -0.22701219, -1.25403476,\n",
       "       -1.54989266, -0.60041094, -1.29876709, -2.54407263, -1.52077937,\n",
       "        0.56938231, -1.38847923,  0.94856429,  0.21916068,  0.53763741,\n",
       "       -0.483953  , -0.24481063,  1.10400212, -0.35310096, -0.88286704,\n",
       "        0.74152726,  0.29003257,  1.08094084,  0.70721066,  1.63394225,\n",
       "        0.70530182, -0.29778716, -0.84324068,  1.37909079, -0.70705599,\n",
       "        1.30713356,  0.26339796, -0.57403082, -1.44039404, -1.03593075,\n",
       "       -0.21261756,  0.88047528,  0.20621392,  0.40647823, -0.03818775,\n",
       "       -0.05348476, -1.69562638, -1.55464637,  0.20927456, -0.85950965,\n",
       "        0.24430278,  1.92697787, -1.39320064, -1.67942941, -0.75467837,\n",
       "       -0.1933718 ,  0.35632777, -0.05327565, -2.84187245,  1.1999284 ,\n",
       "       -0.18148756, -0.49709737,  0.32397431, -0.63812035, -0.26698539,\n",
       "       -0.23300005, -0.45152104,  0.10860274,  1.94057691, -0.42762208,\n",
       "       -0.2333844 , -1.38103545, -1.07648134, -0.1387444 ,  0.05620682,\n",
       "        0.19943775, -1.68176913, -0.09220861,  0.06283973,  0.15745552,\n",
       "       -0.6695174 ,  0.88557369,  0.55961812, -1.71891034,  0.25935498,\n",
       "        0.39008722,  0.09629015, -0.34884432,  0.07141742, -1.49303508,\n",
       "        0.78631037, -0.40955159, -1.11830616, -0.55440098,  0.26420224,\n",
       "       -4.69007254,  1.3989954 ,  3.93796396,  4.0497036 , -1.14447463,\n",
       "        3.33132792, -3.37408662,  1.71937537, -0.30640507,  2.26373076,\n",
       "       -0.802661  ,  0.6772294 ,  2.14062357,  0.87663037, -1.0238961 ,\n",
       "        1.19073498, -2.99887466,  1.9413842 , -0.15227975, -5.27076197,\n",
       "        1.45242131,  0.57341498, -1.138394  ,  1.97815418,  0.04788882,\n",
       "       -0.30670059,  1.54309714,  0.79103363, -1.77176905,  1.92795408,\n",
       "        0.71811008, -0.52918369, -0.19921255,  0.30805093,  2.22163129,\n",
       "        1.22239029, -3.22028112,  4.22672653, -4.5019846 , -2.67574883,\n",
       "       -0.09052869,  0.08295289, -1.73235655,  1.21253633, -0.08308861,\n",
       "       -1.01581621, -3.28605843,  0.5611757 , -2.48348427,  2.05668592,\n",
       "        2.35911894,  1.64962578, -3.92566895,  1.13029456,  2.03636169,\n",
       "       -1.330446  , -3.58890533, -2.58010459,  2.44298029,  0.42626819,\n",
       "        1.3283118 ,  5.00331688, -4.07117224,  2.31880498,  0.43530071,\n",
       "       -3.01922369, -2.7693131 , -0.87138146,  0.89397478, -2.93741798,\n",
       "        1.11976337,  0.37532836, -1.68834198, -2.09719586, -4.33414125,\n",
       "        0.36032486, -3.57536125, -0.66828835,  2.26553845, -1.20059955,\n",
       "        2.84474945, -2.3710556 ,  1.20949888, -4.3074131 ,  2.47549558,\n",
       "       -2.12089825,  2.55576038,  5.12700224,  0.6635946 ,  2.20709038,\n",
       "       -6.23049927,  1.13088918,  0.6639514 , -1.99152565, -0.11131304,\n",
       "        0.8929776 , -6.1337924 , -2.60050106,  0.39461902,  0.42088124])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1680.9456,  4165.1724,  1533.1951,  7199.9072,  5070.6938,  2345.7065,\n",
       "         9250.3389,  5203.1479,  4259.1182,   430.9177,  2109.5708,  3651.4397,\n",
       "         4260.5000,  4897.7485,   176.8114,  5829.6890,  6614.3052,  -509.7720,\n",
       "        -2753.9927, -7278.3423,  4718.3066,   941.2676,  4904.0347,  5274.8608,\n",
       "         5892.4951,   436.8915,  2757.6704,  3857.5964, -3657.4490,  3081.9722,\n",
       "         4571.2686,  3951.9895,  3322.3235,  2367.7852,  3119.3428,  4443.3618,\n",
       "         5116.3521,   619.6339,  7565.7769,  5868.1929,  8455.3555,  4871.3857,\n",
       "         2466.4309,  4256.5620,   367.6161, -2210.6404,  5828.1064,  5478.2021,\n",
       "         3915.7114,  1363.2174,  5922.7593, -4745.3047,  2342.0513,  7956.9697,\n",
       "         3168.1321,  8447.6309,  8911.6738,  4285.8604,  3777.2207,  7706.6494,\n",
       "         9127.8467,  7786.8022,  8250.0674, -7537.2529,  5366.4741,  4783.9878,\n",
       "         7739.3887,  8098.3750,  4422.4053,  2125.5981, -1606.5326,  1737.9966,\n",
       "          168.9280,  8257.9971,  5123.2974,  3121.9326,  -982.4365,  2311.3152,\n",
       "         4881.0098,  2978.5415,  4426.4023,  2493.2834,  7314.3809,  5453.3389,\n",
       "         3994.4419,  4047.3672,  5799.9688,  8206.9287,  2805.7358,  6733.0674,\n",
       "         2668.0916,  4793.7480,  7700.6523,  5221.1758, -7826.3926,  6010.4336,\n",
       "         7525.7705, -1125.0186,  5801.7559,  7319.8706,  5986.3643,  5228.2534,\n",
       "         2791.8892, -1692.6238,  7025.4810,  7016.0259,  -344.9907,   808.5903,\n",
       "         4352.2490,  7106.0942,  7068.4585, -2538.4246, -7120.5171,  4607.4741,\n",
       "         5889.3799,  9227.2188,  7638.6914,  6248.5513,  1578.0997,  3061.6011,\n",
       "         5314.8843,  5737.3750,   711.5245,  5544.2739,  8350.6367,  9368.1777,\n",
       "        10220.8564,  9474.3311,  7893.5645,  7623.9292,  7459.5156,  5726.4902,\n",
       "         1247.0537, -4034.8804,  5197.5176,  7135.0835,  4470.6909,  7557.7466,\n",
       "        -8748.5166,  -650.2751,  6120.7197,  7923.2339,  8389.9902,  5732.5205,\n",
       "         2953.4995,  2939.4880,  3282.8398,  5042.0254, -5563.0737,   632.4487,\n",
       "         8969.8340,  9880.0459,  8615.2490,  2101.6411,    44.9423,  7835.7983,\n",
       "        -4098.9409, -7837.2769,  -574.2739,  -529.7397,   271.8463, -3274.6541,\n",
       "         4900.9072,  8183.2236,  9410.6650,  7907.5303, -1985.5935,  6418.3550,\n",
       "         8374.7676,  5144.4819,  6611.8149,  5528.3013,  1507.5089,  7320.9307,\n",
       "        -1070.1763,  -141.4951,  2355.7268, -1338.9307,  5194.6958,  5269.7217,\n",
       "         8943.0391,  8100.3604,  7740.5312,  4250.5586,  6641.6392,  3989.6467,\n",
       "         6287.7285,  6375.7900,  6364.9185,  4919.1338, -8603.7275,  -438.5869,\n",
       "         4644.9214,  5371.5933,  5673.3896,  4598.4043, -7203.2158, -2655.4031,\n",
       "         6204.5015,  -972.7198], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_from_noise = generator.forward(*FIXED_NOISE)\n",
    "generated_from_noise[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  7.0963, -11.9760,   5.4840, -19.7117, -23.0775,  21.7153, -23.2881,\n",
       "         24.9540, -15.4287, -32.6535, -10.1396,   3.3282,  -1.3652, -24.7476,\n",
       "        -25.1753,  16.1176,  24.7012, -28.7686,  16.9252,  21.7108, -26.1659,\n",
       "         22.7772, -19.5178, -23.4715, -16.4449,  -8.1711, -16.6197,   6.1620,\n",
       "        -21.4484,  14.2393,  -0.2494,  11.7436,  26.6337,  17.5578,  18.1485,\n",
       "        -17.2153, -28.8621, -18.6385,  10.2998, -15.5160, -27.7789,  14.9284,\n",
       "        -11.5600, -28.3599,  28.8360,  19.2762, -21.3333,   4.7429,  -4.4666,\n",
       "         -3.1476,  18.6968,  21.6732,   6.7316,  15.2681, -27.4084,  24.6265,\n",
       "        -18.3698, -17.4179,  17.7174,  -7.1444,  15.4470, -16.7280, -18.2686,\n",
       "        -30.3386, -16.5235, -25.2640, -18.6485,  22.6935, -31.2311,  -9.0147,\n",
       "         -9.7018, -22.6661,  20.5342,   1.3248,  16.8969,  -1.4471,  15.9335,\n",
       "        -22.8307,  24.3155, -26.6364,  25.9134,   6.3582,  18.4957,   4.4277,\n",
       "        -27.1249, -26.8022,  26.8764,  23.6177,  13.6043,   8.3461, -23.5579,\n",
       "         26.1453,  18.9115,   5.9047,  -8.1245,  17.4799, -23.3480, -10.1511,\n",
       "         13.4430,   0.0475,  26.4442, -27.0120,  16.9350,  -0.2435, -20.5496,\n",
       "         25.6316,  10.5984, -22.0979,   1.2782,  -1.4418,  22.2696,  17.2056,\n",
       "         24.6371,  25.5180,  27.0399,  10.9808, -23.5804, -23.4885, -24.3431,\n",
       "          6.3161,  13.7511,  11.5082, -18.4555,  12.0915,  -8.5781, -11.4881,\n",
       "        -28.8242,  14.8045, -16.9305,  20.6519, -12.8138, -26.7478, -27.1614,\n",
       "        -20.3957,   1.8846,  -3.4682, -31.5951, -19.3093,  24.6838, -25.7231,\n",
       "        -29.3075, -27.2908, -22.0804,  25.0239,  23.9171,   3.0910,  24.9802,\n",
       "         30.6782,  28.3445,   8.9330,  26.7070, -25.8536, -24.0716, -16.8975,\n",
       "         -9.2889,  22.6261,  22.0174, -29.3731,  -4.4873,  -4.6144, -20.7072,\n",
       "        -24.4517, -22.1645,  22.5991,  23.3625,  28.5100,  18.0832, -24.1366,\n",
       "        -23.0591,  20.6375,   6.5986, -21.9034,   3.5020,  26.5849, -27.8897,\n",
       "        -23.0799,  -3.8457,  26.5575, -16.3655,  22.6165,  25.2812,  26.8135,\n",
       "         30.4961, -13.0175, -19.3609,  28.1881,  26.2497, -28.8045, -29.6664,\n",
       "          5.2704,  17.0140,   1.4731, -14.4815, -20.4733, -23.8451, -22.1295,\n",
       "         24.8183,   4.0379, -19.4852,  23.3704], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_from_noise = generator.forward(*FIXED_NOISE)\n",
    "generated_from_noise[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
